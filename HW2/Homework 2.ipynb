{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be building several varieties of language models.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in PyTorch:\n",
    "\n",
    "1. A trigram model with linear-interpolation. $$p(y_t | y_{1:t-1}) =  \\alpha_1 p(y_t | y_{t-2}, y_{t-1}) + \\alpha_2 p(y_t | y_{t-1}) + (1 - \\alpha_1 - \\alpha_2) p(y_t) $$\n",
    "2. A neural network language model (consult *A Neural Probabilistic Language Model* http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
    "3. An LSTM language model (consult *Recurrent Neural Network Regularization*, https://arxiv.org/pdf/1409.2329.pdf) \n",
    "4. Your own extensions to these models...\n",
    "\n",
    "\n",
    "Consult the papers provided for hyperparameters.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import numpy as np\n",
    "import time\n",
    "from utils import variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use of this problem is known as the Penn Treebank (http://aclweb.org/anthology/J93-2004). It is the most famous dataset in NLP and includes a large set of different types of annotations. We will be using it here in a simple case as just a language modeling dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we input our data. Here we will use the first 10k sentences of the standard PTB language modeling split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path='../HW2/',\n",
    "    train=\"shuffled_train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "def shuffle_train_txt_file(input_filename, output_filename):\n",
    "    with open(input_filename, 'r') as ifile:\n",
    "        text = shuffle(ifile.read().split('\\n'))\n",
    "    with open(output_filename, 'w') as ofile:\n",
    "        ofile.write('\\n'.join(text))\n",
    "\n",
    "def rebuild_iterators():\n",
    "    if 'shuffled_train.txt' not in os.listdir():\n",
    "        shuffle_train_txt_file('shuffled_train.txt', 'shuffled_train.txt')\n",
    "    else:\n",
    "        shuffle_train_txt_file('train.txt', 'shuffled_train.txt')        \n",
    "    train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "        path='../HW2/',\n",
    "        train=\"shuffled_train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "    train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "        (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "    return train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format for language modeling is strange. We pretend the entire corpus is one long sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 1\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the vocab itself. (This dataset has unk symbols already, but torchtext adds its own.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 10001\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When debugging you may want to use a smaller vocab size. This will run much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "    len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batching is done in a strange way for language modeling. Each element of the batch consists of `bptt_len` words in order. This makes it easy to run recurrent models like RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "companies listed below reported quarterly profit substantially different from the average of analysts ' estimates <eos> sterling was quoted at $ N up from $ N late tuesday <eos> mr. seidman said\n",
      "yesterday for example that sen. dennis <unk> d. ariz. who received $ N in contributions from mr. keating <unk> mr. seidman to request that he push for a sale of lincoln before\n",
      "it would be seized <eos> the canadian government previously said merieux 's bid did n't offer enough net benefit to canada to be approved and gave merieux an until <unk> to submit\n",
      "additional information <eos> belgium was closed for two days france closed for a couple of hours germany was stuck <eos> international copyright secured <eos> a small yield premium over comparable treasurys and\n",
      "a lack of liquidity is <unk> dealers ' efforts to drum up interest in the so-called bailout bonds <eos> changing legislation has opened the field to thousands of <unk> soviet players many\n",
      "who promise more than they can deliver <eos> a ups spokesman said that although none of the company 's terminals trucks or airplanes were damaged in the quake road <unk> and power\n"
     ]
    }
   ],
   "source": [
    "for j, batch in enumerate(train_iter):\n",
    "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 0].data]))\n",
    "    if j > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what these batches look like. Each is a string of length 32. Sentences are ended with a special `<eos>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch [max bptt length, batch size] torch.Size([32, 10])\n",
      "Second in batch Variable containing:\n",
      "  146\n",
      "    3\n",
      "  486\n",
      "   55\n",
      "    5\n",
      "    2\n",
      "   91\n",
      " 5652\n",
      "    0\n",
      "   17\n",
      "  440\n",
      "  136\n",
      "  268\n",
      "  184\n",
      "    8\n",
      "    2\n",
      " 2091\n",
      " 7884\n",
      "  120\n",
      "    6\n",
      "    7\n",
      "   36\n",
      "  242\n",
      "    5\n",
      "   13\n",
      "    4\n",
      "   49\n",
      "   70\n",
      "   41\n",
      "    3\n",
      " 2553\n",
      "   16\n",
      "[torch.LongTensor of size 32]\n",
      "\n",
      "Converted back to string:  investment <eos> despite one of the most devastating <unk> on record net cash income in the farm belt rose to a new high of $ N billion last year <eos> northeast said\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it) \n",
    "print(\"Size of text batch [max bptt length, batch size]\", batch.text.size())\n",
    "print(\"Second in batch\", batch.text[:, 2])\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next batch will be the continuation of the previous. This is helpful for running recurrent neural networks where you remember the current state when transitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted back to string:  it would <unk> its request and still hopes for an <unk> review by the ferc so that it could complete the purchase by next summer if its bid is the one approved\n"
     ]
    }
   ],
   "source": [
    "batch = next(it)\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 2].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no separate labels. But you can just use an offset `batch.text[1:]` to get the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2196017/2196017 [11:18<00:00, 3235.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings size  torch.Size([10001, 300])\n",
      "Word embedding of 'follows', first 10 dim  \n",
      " 0.2057\n",
      " 0.1047\n",
      "-0.3900\n",
      "-0.1086\n",
      "-0.0722\n",
      "-0.1184\n",
      "-0.1109\n",
      " 0.1917\n",
      " 0.4781\n",
      " 2.0576\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "TEXT.vocab.load_vectors(vectors=GloVe())\n",
    "\n",
    "print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])# Build the vocabulary with word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 3 different torch models that take in batch.text and produce a distribution over the next word. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw2-s18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the final Kaggle test, we will have you do a next word prediction task. We will provide a 10 word prefix of sentences, and it is your job to predict 10 possible next word candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sample Kaggle submission, let us build a simple unigram model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "count = Counter()\n",
    "for b in iter(train_iter):\n",
    "    count.update(b.text.view(-1).data.tolist())\n",
    "count[TEXT.vocab.stoi[\"<eos>\"]] = 0\n",
    "predictions = [TEXT.vocab.itos[i] for i, c in count.most_common(20)]\n",
    "with open(\"sample.txt\", \"w\") as fout: \n",
    "    print(\"id,word\", file=fout)\n",
    "    for i, l in enumerate(open(\"input.txt\"), 1):\n",
    "        print(\"%d,%s\"%(i, \" \".join(predictions)), file=fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head sample.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric we are using is mean average precision of your 20-best list. \n",
    "\n",
    "$$MAP@20 = \\frac{1}{|D|} \\sum_{u=1}^{|D|} \\sum_{k=1}^{20} Precision(u, 1:k)$$\n",
    "\n",
    "Ideally we would use log-likelihood or ppl as discussed in class, but this is the best Kaggle gives us. This takes into account whether you got the right answer and how highly you ranked it. \n",
    "\n",
    "In particular, we ask that you do not game this metric. Please submit *exactly 20* unique predictions for each example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always you should put up a 5-6 page write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(t.nn.Module):\n",
    "    def __init__(self, context_size, embeddings):\n",
    "        super(CNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab_size = embeddings.size(0)\n",
    "        self.embed_dim = embeddings.size(1)\n",
    "        \n",
    "        self.w = t.nn.Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.w.weight = t.nn.Parameter(embeddings, requires_grad=False)\n",
    "        \n",
    "        self.conv = t.nn.Conv1d(self.embed_dim, self.vocab_size, context_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xx = self.w(x).transpose(2,1)\n",
    "        xx = self.conv(xx)\n",
    "        return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_size = 5\n",
    "cnn = CNN(context_size, TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "loss(x, class) = -log(exp(x[class]) / (\\sum_j exp(x[j])))\n",
    "               = -x[class] + log(\\sum_j exp(x[j]))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def criterion(pred, true):\n",
    "    l = 0\n",
    "    for k in range(pred.size(2)):\n",
    "        pred_ = pred[:,:,k:k+1].squeeze()\n",
    "        true_ = true[:, k:k+1].squeeze()\n",
    "        l += F.cross_entropy(pred_, true_)\n",
    "    return l / pred.size(2)\n",
    "\n",
    "def accuracy(pred, true):\n",
    "    pred_ = t.max(pred, 1)[1]\n",
    "    return t.mean((pred_==true).float()).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "lr = 1e-3\n",
    "optimizer = t.optim.Adam(filter(lambda p: p.requires_grad, cnn.parameters()), lr=lr)\n",
    "\n",
    "train_accuracies = []\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2905 10\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter), train_iter.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for _ in range(N_EPOCHS):\n",
    "    train_iter.shuffle = True\n",
    "    train_iter.init_epoch()\n",
    "    for i, next_batch in tqdm(enumerate(train_iter)):\n",
    "        if i == 0:\n",
    "            current_batch = next_batch\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # context for starting words\n",
    "            if i > 1:\n",
    "                starting_words = last_batch.text.transpose(0,1)[:, -context_size:]\n",
    "            else:\n",
    "                starting_words = t.zeros(current_batch.text.size(1), context_size).float()\n",
    "            x = t.cat([variable(starting_words, to_float=False).long(), current_batch.text.transpose(0,1).long()], 1)\n",
    "            \n",
    "            # you need the next batch first word to know what the target of the last word of the current batch is\n",
    "            ending_word = next_batch.text.transpose(0,1)[:, :1]\n",
    "            target = t.cat([current_batch.text.transpose(0,1)[:, 1:], ending_word], 1)\n",
    "            \n",
    "            # backprop\n",
    "            pred = cnn(x)[:, :, :-1]  # don't take prediction for the first word of the next batch, it is done in the next batch\n",
    "            \n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_accuracies.append(accuracy(pred, target))\n",
    "            train_losses.append(loss.data.numpy()[0])\n",
    "            \n",
    "            # update batches\n",
    "            last_batch = current_batch\n",
    "            current_batch = next_batch\n",
    "    print('--------------------------\\nEpoch %d took %.3fs' % (_, time.time() - t0))\n",
    "    print(\"For epoch %d, train accuracy is : %.3f\" % (_, np.mean(train_accuracies[-len(train_iter):])))\n",
    "    print(\"For epoch %d, train loss is : %.3f\" % (_, np.mean(train_losses[-len(train_iter):])))\n",
    "    t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(iterator, model_str, context_size, cuda=True):\n",
    "    for i, next_batch in enumerate(iterator):\n",
    "        if i == 0:\n",
    "            current_batch = next_batch\n",
    "        else:\n",
    "            if model_str == 'NNLM':\n",
    "                if context_size is not None:\n",
    "                    if i > 1:\n",
    "                        starting_words = last_batch.text.transpose(0, 1)[:, -context_size:]\n",
    "                    else:\n",
    "                        starting_words = t.zeros(current_batch.text.size(1), context_size).float()\n",
    "                    x = t.cat([variable(starting_words, to_float=False, cuda=cuda).long(), current_batch.text.transpose(0, 1).long()], 1)\n",
    "                else:\n",
    "                    raise ValueError('`context_size` should not be None')\n",
    "            else:\n",
    "                x = current_batch.text.transpose(0, 1).long()\n",
    "\n",
    "            target = current_batch.text.transpose(0, 1)\n",
    "\n",
    "            last_batch = current_batch\n",
    "            current_batch = next_batch\n",
    "\n",
    "            yield x, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "x\n",
      "Variable containing:\n",
      "  9998   9999  10000      3   9257      0      4     73    394     34   2134\n",
      "    31    295   4901     13      4     49      3      0      0     25   2471\n",
      "     0     20      2    273   7821     17      9    117   2815    969      6\n",
      "    40     14   3582      9      0   7238     10    391     45    487      0\n",
      "    11    271     44     13      4      8      2    380     14   4073     21\n",
      "    13      4     49    156      4    121   1188    363    547     35   2130\n",
      "  2258     24   1891      0      6      7      0     12    212     62    608\n",
      "    38     23     74     11    864     12   3959      9   1197    351      3\n",
      "   155      3      7     36     93     61    112   7859   1555   1827   2786\n",
      "  3464     51     44      4      4      5      2     48     63      3   4332\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "y\n",
      "Variable containing:\n",
      "  9998   9999  10000      3   9257      0      4     73    394     34   2134\n",
      "    31    295   4901     13      4     49      3      0      0     25   2471\n",
      "     0     20      2    273   7821     17      9    117   2815    969      6\n",
      "    40     14   3582      9      0   7238     10    391     45    487      0\n",
      "    11    271     44     13      4      8      2    380     14   4073     21\n",
      "    13      4     49    156      4    121   1188    363    547     35   2130\n",
      "  2258     24   1891      0      6      7      0     12    212     62    608\n",
      "    38     23     74     11    864     12   3959      9   1197    351      3\n",
      "   155      3      7     36     93     61    112   7859   1555   1827   2786\n",
      "  3464     51     44      4      4      5      2     48     63      3   4332\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "i 1\n",
      "x\n",
      "Variable containing:\n",
      " 7683     0     4    73   394     9   338   142     5  2478   658\n",
      "  267     3    24     0    25   720    71     9   152   299   291\n",
      "    7   263  4923     5   243     2   508    10   737  1071  6034\n",
      "  348     6  2788  1053     3   660  4491    45    11     0   559\n",
      "   56     4    49  4995     8   746   904     9   327     6  1270\n",
      "    6    99     3    40  1182    80    26   453   667  4810  4273\n",
      " 1131   128  2477  1202     3    69   827     2  2484    57   160\n",
      "  111    35     7   405  2101   235     3    15   404    26   584\n",
      "    9    82     3    15    60  2369    13     4     8   193  3933\n",
      "   17  1944  1010   540    21  5434     2   455     0  3521   310\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "y\n",
      "Variable containing:\n",
      " 7683     0     4    73   394     9   338   142     5  2478   658\n",
      "  267     3    24     0    25   720    71     9   152   299   291\n",
      "    7   263  4923     5   243     2   508    10   737  1071  6034\n",
      "  348     6  2788  1053     3   660  4491    45    11     0   559\n",
      "   56     4    49  4995     8   746   904     9   327     6  1270\n",
      "    6    99     3    40  1182    80    26   453   667  4810  4273\n",
      " 1131   128  2477  1202     3    69   827     2  2484    57   160\n",
      "  111    35     7   405  2101   235     3    15   404    26   584\n",
      "    9    82     3    15    60  2369    13     4     8   193  3933\n",
      "   17  1944  1010   540    21  5434     2   455     0  3521   310\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "i 2\n",
      "x\n",
      "Variable containing:\n",
      " 6093  4242  6037    31   989     7   242   761     5  1016  2787\n",
      "    3   476   523     4     4     3     2   863    54     9   244\n",
      "  180    27   170     6   465   609     0    42    43    26   263\n",
      "   50  5010     0    20   201    20    24  4491    10     0     8\n",
      "  293     0  3341    11     2  6202    14    33   986   122  3927\n",
      " 4960    30   180    27   501    56     2   265   275    42     0\n",
      " 2697     3    43    69    35   925    74   151   608     0    67\n",
      "   38   213    37     0    34    26     0    23   682     3    19\n",
      " 2402  7509     9     0  2010  3933     2    76   946    16    39\n",
      "    0     6   499     2   540   199    24     0    10   265  2285\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "y\n",
      "Variable containing:\n",
      " 6093  4242  6037    31   989     7   242   761     5  1016  2787\n",
      "    3   476   523     4     4     3     2   863    54     9   244\n",
      "  180    27   170     6   465   609     0    42    43    26   263\n",
      "   50  5010     0    20   201    20    24  4491    10     0     8\n",
      "  293     0  3341    11     2  6202    14    33   986   122  3927\n",
      " 4960    30   180    27   501    56     2   265   275    42     0\n",
      " 2697     3    43    69    35   925    74   151   608     0    67\n",
      "   38   213    37     0    34    26     0    23   682     3    19\n",
      " 2402  7509     9     0  2010  3933     2    76   946    16    39\n",
      "    0     6   499     2   540   199    24     0    10   265  2285\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "i 3\n",
      "x\n",
      "Variable containing:\n",
      " 4053     0   497    15  6886     2     0    23   114  2653  8069\n",
      "    4     4     3     2   977   159    17   353   363    19   381\n",
      "   17     2    36    93    61   112  6551  6442     4     6     4\n",
      "    6    26  4301     6   189  7238    10  1367    85     9  1683\n",
      "    5     0  5208     3     2     0     5     2   455   159   206\n",
      "  910   569   285     3     7   569    11  1895     2   309   271\n",
      "   11    29  1154   202     5     2  1872    17    24  1891    10\n",
      " 2997    27  3316    19   134  4578     3    19     0     2   275\n",
      " 3390  1572     0     2    76   946    10   752    19     0     0\n",
      "    7  1541   455     0  3001   232     6   318     7   304  5468\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "y\n",
      "Variable containing:\n",
      " 4053     0   497    15  6886     2     0    23   114  2653  8069\n",
      "    4     4     3     2   977   159    17   353   363    19   381\n",
      "   17     2    36    93    61   112  6551  6442     4     6     4\n",
      "    6    26  4301     6   189  7238    10  1367    85     9  1683\n",
      "    5     0  5208     3     2     0     5     2   455   159   206\n",
      "  910   569   285     3     7   569    11  1895     2   309   271\n",
      "   11    29  1154   202     5     2  1872    17    24  1891    10\n",
      " 2997    27  3316    19   134  4578     3    19     0     2   275\n",
      " 3390  1572     0     2    76   946    10   752    19     0     0\n",
      "    7  1541   455     0  3001   232     6   318     7   304  5468\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "i 4\n",
      "x\n",
      "Variable containing:\n",
      "   11   559  6093  3575  1899   667     0     8    28     0  4242\n",
      "    4     4     4   493     3   825   927   212   477   248    12\n",
      "   32  8041     5     7   375    37    44    13     4    12   216\n",
      "    6  2125     0    20     7  1362    37    50   465     8  1479\n",
      "  330    14   202     5     7  1530     6  3986    85  6023  5372\n",
      " 9643   128     6     3     4  4493   740  1410     9  2206  1800\n",
      "  419    33    26   831     8    28   627    42    14  3189    18\n",
      "   19  1990     2   435  1278   147    94  2542    87     4    37\n",
      "  272    97    42    31    59     7  1886  2560     8     2  1764\n",
      " 2864     6   894  3369  9325     3    30  2052   826     6   270\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n",
      "y\n",
      "Variable containing:\n",
      "   11   559  6093  3575  1899   667     0     8    28     0  4242\n",
      "    4     4     4   493     3   825   927   212   477   248    12\n",
      "   32  8041     5     7   375    37    44    13     4    12   216\n",
      "    6  2125     0    20     7  1362    37    50   465     8  1479\n",
      "  330    14   202     5     7  1530     6  3986    85  6023  5372\n",
      " 9643   128     6     3     4  4493   740  1410     9  2206  1800\n",
      "  419    33    26   831     8    28   627    42    14  3189    18\n",
      "   19  1990     2   435  1278   147    94  2542    87     4    37\n",
      "  272    97    42    31    59     7  1886  2560     8     2  1764\n",
      " 2864     6   894  3369  9325     3    30  2052   826     6   270\n",
      "[torch.LongTensor of size 10x11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,(x,y) in enumerate(data_generator(train_iter, 'NNLM', 5, False)):\n",
    "    print('i',i)\n",
    "    print('x')\n",
    "    print(x[:,-11:])\n",
    "    print('y')\n",
    "    print(y[:,-11:])\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# t.max(pred, 1)[1]\n",
    "# # t.mean((pred==true).float()).data.numpy()[0]\n",
    "accuracy(pred,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter.init_epoch()\n",
    "for i, next_batch in enumerate(train_iter):\n",
    "    if i == 0:\n",
    "        current_batch = next_batch\n",
    "    else:\n",
    "        # context for starting words. NO NEED FOR LSTM/RNN.\n",
    "        if i > 1:\n",
    "            starting_words = last_batch.text.transpose(0,1)[:, -context_size:]\n",
    "        else:\n",
    "            starting_words = t.zeros(current_batch.text.size(1), context_size).float()\n",
    "        x = t.cat([variable(starting_words, to_float=False).long(), current_batch.text.transpose(0,1).long()], 1)\n",
    "\n",
    "        # you need the next batch first word to know what the target of the last word of the current batch is\n",
    "        ending_word = next_batch.text.transpose(0,1)[:, :1]\n",
    "        target = t.cat([current_batch.text.transpose(0,1)[:, 1:], ending_word], 1)\n",
    "        \n",
    "        last_batch = current_batch\n",
    "        current_batch = next_batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
