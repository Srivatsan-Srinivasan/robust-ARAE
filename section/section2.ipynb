{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: RNNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "1. Build a simple RNN classifier\n",
    "2. Learn about PyTorch's in-built RNN modules (LSTM etc.)\n",
    "\n",
    "(Roughly follows http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Building an RNN sentiment classifier\n",
    "#### Part 1.1: Generating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll generate some toy data. The task will be to recall an integer at a certain position in a sequence. \n",
    "For a sequence a<sub>1</sub> a<sub>12</sub> a<sub>3</sub> a<sub>4</sub> a<sub>5</sub> the output might be a<sub>3</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of training examples\n",
    "n_train = 2000\n",
    "\n",
    "# number of validation examples\n",
    "n_val = 1000\n",
    "\n",
    "# length of each sequence\n",
    "n_length = 10\n",
    "\n",
    "# examples per batch\n",
    "n_batch = 32\n",
    "\n",
    "# size of the vocabulary\n",
    "n_vocab = 20\n",
    "\n",
    "# position to be recalled\n",
    "answer_pos = n_length-1\n",
    "\n",
    "# generate random sequences\n",
    "train_seq = Variable(torch.Tensor(n_train, n_length).random_(0, n_vocab).long())\n",
    "val_seq = Variable(torch.Tensor(n_val, n_length).random_(0, n_vocab).long())\n",
    "\n",
    "# choose the correct labels\n",
    "train_labels = train_seq.clone()[:, answer_pos]\n",
    "val_labels = val_seq.clone()[:, answer_pos]\n",
    "\n",
    "# group data into batches\n",
    "train_iter = []\n",
    "for i in range(0, n_train, n_batch):\n",
    "    batch_seq = train_seq[i:i+n_batch]\n",
    "    batch_labels = train_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        train_iter.append([batch_seq, batch_labels])\n",
    "    \n",
    "val_iter = []\n",
    "for i in range(0, n_val, n_batch):\n",
    "    batch_seq = val_seq[i:i+n_batch]\n",
    "    batch_labels = val_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        val_iter.append([batch_seq, batch_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2 Build the model (version 1)\n",
    "\n",
    "The RNN module will be a PyTorch model like any other, with init a forward functions. This network:\n",
    "1. Takes as input the word at a particular point in the sequence, as well as the hidden state at the previous state of the network\n",
    "2. Uses nn.Embedding to get a vector for the word\n",
    "3. Concatenate the embedding and the hidden state\n",
    "4. Apply a linear layer to get the next hidden state\n",
    "5. Apply a linear layer to get the output\n",
    "6. Output both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        combined = torch.cat((embedded, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vector\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    for i in range(batch.size()[1]):\n",
    "        output, hidden = model(batch[:, i], hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-bd49eb84c163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# initialize the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-912781308f8d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, output_size, vocab_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi2h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi2o\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'dim'"
     ]
    }
   ],
   "source": [
    "# size of the hidden vector\n",
    "n_hidden = 3\n",
    "\n",
    "# initialize the network\n",
    "rnn = RNN(n_vocab, n_hidden, n_vocab, n_vocab)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .05\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.4: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model is similar to training it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_batch(batch, label):\n",
    "    if (batch.size()[0] != n_batch):\n",
    "        return 0, 0\n",
    "    \n",
    "    # initialize hidden state\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "    \n",
    "    # calculate forward pass\n",
    "    for i in range(batch[0].size()[0]):\n",
    "        output, hidden = rnn(batch[:, i], hidden)\n",
    "        \n",
    "    # calculate predictions\n",
    "    _, pred = output.max(1)\n",
    "\n",
    "    # calculate number of correct predictions\n",
    "    correct = (pred == label).long().sum().data[0]\n",
    "    return correct, n_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calculate the total score by looping through the batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction correct:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Fraction correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Using PyTorch RNN modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's RNN capabilities live [here](http://pytorch.org/docs/master/nn.html#recurrent-layers). We can use it as follows (note that the input is batched along the **second** dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.4608 -0.1072 -0.0000 -0.0044  0.1335  0.0743  0.2502 -0.2156  0.2602\n",
      " -0.4931 -0.2171 -0.1714 -0.5364 -0.0910  0.4206  0.1661 -0.0412 -0.3391\n",
      " -0.4461 -0.1393  0.2427 -0.1197 -0.4182  0.0320  0.2920  0.2345 -0.4173\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.1045  0.5604  0.4094 -0.1384 -0.3334 -0.0955 -0.1117 -0.0656  0.0648\n",
      " -0.3996 -0.0585  0.1020  0.1063  0.1838 -0.1989  0.1435 -0.0395  0.3219\n",
      "  0.4321 -0.0071 -0.1174  0.1073 -0.4654 -0.0068  0.4993 -0.0367  0.2381\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0390 -0.4322\n",
      "  0.1172 -0.1978\n",
      "  0.1186  0.0487\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.1953  0.0525  0.1024 -0.0282 -0.0380  0.0456  0.1338 -0.1059  0.1165\n",
      " -0.2181 -0.0132 -0.0461 -0.1994 -0.0268  0.1971  0.1365  0.0736 -0.2742\n",
      " -0.1464  0.0155  0.2230 -0.0221 -0.2437  0.0604  0.1570  0.2687 -0.2530\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.0131  0.3302  0.3256  0.0515 -0.2773 -0.1092 -0.1512 -0.1325  0.0694\n",
      " -0.2193  0.1088  0.1282  0.1127  0.0095 -0.0725 -0.0710 -0.1267  0.1606\n",
      "  0.1226  0.0120  0.0498  0.1097 -0.2723 -0.0396  0.1208 -0.0819  0.2097\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.1047 -0.1724\n",
      "  0.1490 -0.1185\n",
      "  0.1553 -0.0263\n",
      "\n",
      "(2 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0993  0.1124  0.1512 -0.0134 -0.1181  0.0047  0.0983 -0.0245  0.0484\n",
      " -0.1183  0.0595  0.0805 -0.0677 -0.0878  0.0594  0.0771  0.1181 -0.1781\n",
      "  0.0132  0.0854  0.1925  0.0240 -0.2108  0.0179  0.1066  0.2171 -0.1684\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.0963  0.2176  0.2724  0.1282 -0.1362 -0.0363 -0.1728 -0.1666  0.0857\n",
      " -0.1724  0.1408  0.1504  0.1290 -0.0369 -0.0697 -0.1208 -0.1105  0.1235\n",
      "  0.0014  0.0502  0.1303  0.1418 -0.1884 -0.0410 -0.0466 -0.1038  0.1605\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.1544 -0.0915\n",
      "  0.1365 -0.0663\n",
      "  0.1522 -0.0560\n",
      "\n",
      "(3 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0629  0.1215  0.1587  0.0013 -0.1491 -0.0233  0.0687  0.0290 -0.0157\n",
      " -0.0713  0.0890  0.1394  0.0054 -0.0859  0.0116  0.0607  0.1240 -0.1317\n",
      "  0.0849  0.1314  0.1830  0.0340 -0.2085 -0.0150  0.0821  0.1794 -0.1193\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1158  0.1462  0.2241  0.1636 -0.0677 -0.0195 -0.1887 -0.1560  0.1032\n",
      " -0.1409  0.1697  0.1467  0.1045 -0.0479 -0.0548 -0.1451 -0.1152  0.0929\n",
      " -0.0737  0.0649  0.1643  0.1643 -0.1116 -0.0264 -0.1216 -0.1230  0.1269\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.1624 -0.0613\n",
      "  0.1453 -0.0684\n",
      "  0.1424 -0.0546\n",
      "\n",
      "(4 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0346  0.1172  0.1621  0.0153 -0.1416 -0.0212  0.0574  0.0686 -0.0593\n",
      " -0.0346  0.1050  0.1669  0.0329 -0.1074 -0.0154  0.0577  0.1178 -0.1016\n",
      "  0.1145  0.1477  0.1841  0.0427 -0.2074 -0.0299  0.0668  0.1269 -0.0782\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1090  0.1395  0.1752  0.1565 -0.0507 -0.0304 -0.1883 -0.1418  0.1036\n",
      " -0.1334  0.1674  0.1573  0.1117 -0.0294 -0.0381 -0.1590 -0.1182  0.0973\n",
      " -0.1121  0.0867  0.1540  0.1983 -0.0711 -0.0414 -0.1507 -0.1368  0.1322\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.1559 -0.0620\n",
      "  0.1526 -0.0728\n",
      "  0.1369 -0.0827\n",
      "[torch.FloatTensor of size 5x3x20]\n",
      " (Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.2161  0.0135 -0.0625 -0.0461 -0.0730 -0.1195 -0.0818  0.0838  0.0067\n",
      " -0.1108  0.0545 -0.2560  0.0046 -0.0028 -0.1220 -0.0938  0.1050  0.0419\n",
      " -0.2433 -0.0487 -0.2747  0.5158  0.2537 -0.0882  0.3364  0.0025 -0.0439\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1027 -0.1421 -0.0644  0.0313 -0.1806  0.0128 -0.0721  0.2398 -0.0712\n",
      " -0.0655 -0.1168  0.0351 -0.0690 -0.1241 -0.0178  0.0128  0.2629 -0.0019\n",
      "  0.0091 -0.0343  0.1331 -0.1660 -0.1968  0.1173 -0.2399  0.3391 -0.0148\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0220 -0.1439\n",
      " -0.0625  0.0656\n",
      " -0.2111 -0.0360\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0346  0.1172  0.1621  0.0153 -0.1416 -0.0212  0.0574  0.0686 -0.0593\n",
      " -0.0346  0.1050  0.1669  0.0329 -0.1074 -0.0154  0.0577  0.1178 -0.1016\n",
      "  0.1145  0.1477  0.1841  0.0427 -0.2074 -0.0299  0.0668  0.1269 -0.0782\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.1090  0.1395  0.1752  0.1565 -0.0507 -0.0304 -0.1883 -0.1418  0.1036\n",
      " -0.1334  0.1674  0.1573  0.1117 -0.0294 -0.0381 -0.1590 -0.1182  0.0973\n",
      " -0.1121  0.0867  0.1540  0.1983 -0.0711 -0.0414 -0.1507 -0.1368  0.1322\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.1559 -0.0620\n",
      "  0.1526 -0.0728\n",
      "  0.1369 -0.0827\n",
      "[torch.FloatTensor of size 2x3x20]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4179  0.0217 -0.2095 -0.0786 -0.1205 -0.3023 -0.1275  0.1617  0.0146\n",
      " -0.2468  0.0974 -0.5023  0.0085 -0.0047 -0.2466 -0.1795  0.2789  0.0666\n",
      " -0.5505 -0.1206 -0.5395  0.9582  0.4886 -0.2209  0.5562  0.0092 -0.0650\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.2101 -0.2831 -0.1550  0.0559 -0.7335  0.0283 -0.1539  0.5200 -0.1335\n",
      " -0.2137 -0.2862  0.0857 -0.1342 -0.2407 -0.0364  0.0305  0.4935 -0.0039\n",
      "  0.0493 -0.0996  0.5046 -0.3022 -0.4041  0.2539 -0.4896  0.4515 -0.0279\n",
      "\n",
      "Columns 18 to 19 \n",
      "  -0.0535 -0.2741\n",
      " -0.1566  0.1102\n",
      " -0.5103 -0.0596\n",
      "\n",
      "(1 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0625  0.2442  0.3245  0.0282 -0.2720 -0.0490  0.1461  0.1353 -0.1244\n",
      " -0.0624  0.2111  0.3198  0.0611 -0.1975 -0.0338  0.1453  0.2411 -0.2134\n",
      "  0.2067  0.2926  0.3514  0.0796 -0.4519 -0.0569  0.1597  0.2538 -0.1688\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.2205  0.2765  0.3555  0.3629 -0.1114 -0.0665 -0.3954 -0.3165  0.2018\n",
      " -0.2800  0.3375  0.3223  0.2419 -0.0680 -0.0871 -0.3387 -0.2577  0.1888\n",
      " -0.2286  0.1754  0.3308  0.4456 -0.1816 -0.0905 -0.3359 -0.2791  0.2628\n",
      "\n",
      "Columns 18 to 19 \n",
      "   0.3325 -0.1625\n",
      "  0.3042 -0.1827\n",
      "  0.2606 -0.2229\n",
      "[torch.FloatTensor of size 2x3x20]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_input = 10\n",
    "n_hidden = 20\n",
    "n_layers = 2\n",
    "n_batch = 3\n",
    "n_length = 5\n",
    "rnn = nn.LSTM(n_input, n_hidden, n_layers)\n",
    "input = Variable(torch.randn(n_length, n_batch, n_input))\n",
    "h0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "c0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "output, hn = rnn(input, (h0, c0))\n",
    "print(output, hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a custom module to apply this module to our problem. This module will embed each integer, then apply the LSTM to the sequence, and then apply a linear and a softmax to get probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, vocab_size, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # embed the input integers\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # put the batch along the second dimension\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "        \n",
    "        # apply the LSTM\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # apply the linear and the softmax\n",
    "        output = self.softmax(self.linear(output))\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing are essentially the same as before, except that we no longer need to manually loop in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vectors\n",
    "    hidden = (Variable(torch.zeros(n_layers, n_batch, n_hidden)), Variable(torch.zeros(n_layers, n_batch, n_hidden)))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    output, hidden = model(batch, hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output[answer_pos], label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected r_ [32 x 512], t [32 x 512] and src [3 x 512] to have the same number of elements, but got 16384, 16384 and 1536 elements respectively at d:\\projects\\pytorch\\torch\\lib\\th\\generic/THTensorMath.c:887",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c2a229231f9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0moptim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-1cf79698faaa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optim)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-326ea743717f>\u001b[0m in \u001b[0;36mtrain_batch\u001b[1;34m(model, criterion, optim, batch, label)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# calculate forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-f91e3e28a10b>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# apply the LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# apply the linear and the softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         )\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, weight, hidden)\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mnexth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[0mhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m             \u001b[1;31m# hack to handle LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[1;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mgates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mingate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    814\u001b[0m     \u001b[0m__radd__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m_add\u001b[1;34m(self, other, inplace)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mAdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\autograd\\_functions\\basic_ops.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, a, b, inplace)\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: inconsistent tensor size, expected r_ [32 x 512], t [32 x 512] and src [3 x 512] to have the same number of elements, but got 16384, 16384 and 1536 elements respectively at d:\\projects\\pytorch\\torch\\lib\\th\\generic/THTensorMath.c:887"
     ]
    }
   ],
   "source": [
    "# size of the embeddings and vectors\n",
    "n_embedding = 128\n",
    "n_hidden = 128\n",
    "\n",
    "# number of layers\n",
    "n_layers = 1\n",
    "\n",
    "# initialize LSTM\n",
    "rnn = MyLSTM(n_embedding, n_hidden, n_vocab, n_vocab, n_layers)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .1\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Percent correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
