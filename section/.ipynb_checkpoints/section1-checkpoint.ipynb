{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: PyTorch Tutorial (Tensors, Automatic Differentation, Linear Regression, MNIST Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "1. Learn to work with/manipulate tensors in PyTorch\n",
    "2. Learn about automatic differentiation\n",
    "3. Fit a linear regression model\n",
    "3. Build a simple classifier on MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors, or multidimensional arrays, are fundamental objects that you will be working with in Torch (and deep learning in general). Let's create some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(5,5) #create a 5 x 5 tensor\n",
    "print(x)\n",
    "x = torch.LongTensor(5,2) #often our input data consists of integers (word indices), so it's helpful to use LongTensors\n",
    "print(x)\n",
    "#we can also do other initializations\n",
    "x = torch.randn(3,4) #initialize from standard normal\n",
    "x = torch.ones(3,4) #all ones\n",
    "x = torch.zeros(3,4) #all zeros\n",
    "x = torch.eye(3) # identity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go back between numpy/torch tensors with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy = np.random.randn(5)\n",
    "print(x_numpy)\n",
    "x_torch = torch.from_numpy(x_numpy)\n",
    "print(x_torch)\n",
    "x_numpy2 = x_torch.numpy()\n",
    "print(x_numpy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing torch tensors is essentially identitical to the case in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.45502009987831116\n",
      "\n",
      "-0.4550\n",
      "-1.2912\n",
      "-0.7031\n",
      "-0.2533\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4,3)\n",
    "print(x[0, 0]) \n",
    "print(x[:, 0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to manipulate tensors. Of particular utility is the *view* function, which reshapes a tensor in memory. See [here](http://pytorch.org/docs/master/tensors.html) for more operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.4550  1.1595  0.0354 -1.2912  1.4296 -0.1067\n",
      "-0.7031 -1.0245 -1.9980 -0.2533  0.9971 -1.3582\n",
      "[torch.FloatTensor of size 2x6]\n",
      "\n",
      "\n",
      "-0.4550\n",
      " 1.1595\n",
      " 0.0354\n",
      "-1.2912\n",
      " 1.4296\n",
      "-0.1067\n",
      "-0.7031\n",
      "-1.0245\n",
      "-1.9980\n",
      "-0.2533\n",
      " 0.9971\n",
      "-1.3582\n",
      "[torch.FloatTensor of size 12]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.view(2,6)) #reshape the 4x3 tensor into a 2x6 tensor\n",
    "print(x.view(-1)) # -1 always reshapes to a 1d tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use operations on tensors as in numpy. Operations that have an underscore _ are *in-place* and modify the original tensor. Other operations will create a new tensor in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.3184  0.9785 -0.1754  1.1572  2.2437\n",
      "-0.7606  0.2648  0.8605  0.1015 -1.8445\n",
      " 0.2775  0.8094 -0.4362  0.8147  1.2170\n",
      " 1.0845 -1.3369 -0.0064 -1.3132 -0.8859\n",
      "-0.9587 -0.1473  0.1089  1.6917  0.9154\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      " 0.3184  0.9785 -0.1754  1.1572  2.2437\n",
      "-0.7606  0.2648  0.8605  0.1015 -1.8445\n",
      " 0.2775  0.8094 -0.4362  0.8147  1.2170\n",
      " 1.0845 -1.3369 -0.0064 -1.3132 -0.8859\n",
      "-0.9587 -0.1473  0.1089  1.6917  0.9154\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      " 1.1102 -0.0549 -0.8609  1.7754  3.0535\n",
      "-1.7186  0.8972  1.1294 -0.6460 -2.3654\n",
      " 0.7893  0.0013 -0.4281  0.4174  2.0825\n",
      "-0.7631 -2.0120 -0.1296 -0.6280 -1.1096\n",
      "-0.2246  0.1781  0.6952  1.9689  1.6550\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 5)\n",
    "y = torch.randn(5, 5)\n",
    "z = torch.mm(x, y) #matrix multiply\n",
    "z = 2*x # scalar multiplication\n",
    "z = x + y #addition, creates a new tensor\n",
    "print(x)\n",
    "z = x.add(y) #same\n",
    "print(x)\n",
    "x.add_(y) #modifies x by adding y to it\n",
    "print(x)\n",
    "#there are other operations such as x.mul_(), x.div_() etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so the tensor object is basically the same as a numpy array. What's neat is that you can define **computation graphs** with tensors and backpropagate gradients through this computational graph automatically. This is known as automatic differentation. To do this however, we need to use a `Variable` object which is basically a wrapper around a tensor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.8044\n",
      "-0.9632\n",
      "-0.0991\n",
      " 0.6308\n",
      "-0.0903\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "x = Variable(torch.randn(5), requires_grad=True) #requires grad means that we want to calculate gradients\n",
    "print(x.data) #a tensor object\n",
    "print(x.grad) #the gradient. right now we haven't calculated any gradients so there is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.6273\n",
      " 0.2815\n",
      " 1.3410\n",
      " 1.0517\n",
      " 0.0955\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(5), requires_grad=True) \n",
    "y = x.mul(2) #y is now a variable. almost any operation you can do on tensors you can do on Variables\n",
    "print(y)\n",
    "y.backward(Variable(torch.ones(y.size()))) #this is a tricky concept, but we essentially backprop a vector of ones\n",
    "#to simulate the calculation of dy[i] / dx for all i\n",
    "print(x.grad) #this should be a vector of 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in practice, we almost always calculate the derivatve with respect to a **scalar**. In this case we can simply call `.backward()` without having to backpropagate a 1x1 vector of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.grad.data.zero_() #gradients are always accumulated, so we need to zero them out manually\n",
    "y = x.mul(2).sum() # now y is a scalar. In most cases this would be your average loss\n",
    "y.backward() #this is equivalent to y.backward(Variable(torch.ones(y.size())))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a simple least squares model on a synthetic dataset with gradient descent. First let's generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_points = 1000\n",
    "num_features = 5\n",
    "w_star = torch.randn(num_features) #true weight vector\n",
    "x = torch.randn(num_points, num_features) #input data\n",
    "y = torch.mm(x, w_star.view(5, 1)) #torch.mm expects both inputs to be matrices so we cast w_star to be a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem has an analytic solution which we can calculate directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ols = torch.mm(torch.mm(torch.inverse(torch.mm(x.t(), x)), x.t()), y) #(X^T X)^{-1} X^T Y\n",
    "print(w_ols)\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's see if we can obtain (approximately) the same solution with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.277718544006348\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Variable' object has no attribute 'zero_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8e5faa3db8b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mw_sgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mw_sgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#gradients get accumulated so we have to manually zero them out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_sgd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_sgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#unsqueeze adds an extra dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_sgd\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\SrivatsanPC\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Variable' object has no attribute 'zero_'"
     ]
    }
   ],
   "source": [
    "w_sgd = Variable(torch.randn(num_features), requires_grad=True) #randomly initialize\n",
    "x_sgd = Variable(x) #remember, we need to convert everything to Variables to work with automatic differentiation\n",
    "y_sgd = Variable(y) #we don't need to calculate gradients with respect to these guys\n",
    "\n",
    "num_iters = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(num_iters):\n",
    "    if w_sgd.grad is not None:\n",
    "        w_sgd.grad.zero_() #gradients get accumulated so we have to manually zero them out\n",
    "    y_pred = torch.mm(x_sgd, w_sgd.unsqueeze(1)) #unsqueeze adds an extra dimension\n",
    "    error = (y_sgd - y_pred)**2\n",
    "    error_avg = error.mean()\n",
    "    if i % 10 == 0:\n",
    "        print(i, error_avg.data[0])\n",
    "    error_avg.backward()    \n",
    "    w_sgd.data = w_sgd.data - learning_rate*w_sgd.grad.data\n",
    "        \n",
    "print(w_sgd)\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that worked, but it's a bit annoying to separately define the weights as `Variables` and manually apply matrix-multiplies. Fortunately, torch provides an `nn` package which provides abstractions for almost all of the layers that we will use in the course. Let's see a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "lin = nn.Linear(num_features, 1, bias=False) #this defines a linear layer that goes from num_feature dimensions to 1\n",
    "print(lin.weight) #weight parameter of the linear layer. if bias = True, then we can access bias with lin.bias\n",
    "y_pred = lin(x_sgd) #lin(x_sgd) automatically calls forward on the input x_sgd\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of `nn` layers can be found [here](http://pytorch.org/docs/master/nn.html). You should try to familiarize yourself with pretty much all the `nn` layers. Torch also provides an `optim` package that makes parameter updates easier. We will be working with SGD in this tutorial, but other optimization algorithms (Adam, Adagrad, RMSProp, etc) can be found [here](http://pytorch.org/docs/master/optim.html). Let's try to fit the linear regression model with these abstractions now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-0.0935  0.8687 -0.2312 -1.3896 -1.1695\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(lin.parameters(), lr = learning_rate) #initialize with parameters\n",
    "for i in range(num_iters):\n",
    "    optimizer.zero_grad() #this will zero out the gradients of all your parameters\n",
    "    y_pred = lin(x_sgd)\n",
    "    error = (y_sgd - y_pred)**2\n",
    "    error_avg = error.mean()    \n",
    "    error_avg.backward()    \n",
    "    optimizer.step()    \n",
    "print(lin.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "val_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a loader around tensor datasets so you can create/access mini-batches easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "for (image, label) in train_loader: #iterates through the dataset in mini-batches\n",
    "    print(image.size())\n",
    "    print(label.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we are ready to create our first model, which will be a simple multilayer perceptron. To do this, it helps to define a `class` with all the layers inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP (\n",
      "  (hidden_to_output): Linear (10 -> 10)\n",
      "  (hidden_layers): Sequential (\n",
      "    (0): Linear (784 -> 10)\n",
      "    (1): ReLU ()\n",
      "  )\n",
      "  (logsoftmax): LogSoftmax ()\n",
      ")\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n",
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers = 1, input_dim = 28*28, output_dim = 10, hidden_dim = 10):\n",
    "        super(MLP, self).__init__()                \n",
    "        self.hidden_to_output = nn.Linear(hidden_dim, output_dim)\n",
    "        hidden_layers = []\n",
    "        for l in range(num_layers):\n",
    "            dim = input_dim if l == 0 else hidden_dim #first layer is input to hidden layer\n",
    "            hidden_layers.append(nn.Linear(dim, hidden_dim))\n",
    "            hidden_layers.append(nn.ReLU()) #let's work with relu nonlinearities for now\n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers) #Sequential module will apply the layers in sequence\n",
    "        self.logsoftmax = nn.LogSoftmax() #softmax will turn the output into probabilities, but log is more convnient\n",
    "        \n",
    "    def forward(self, x): #MLP(x) is shorthand for MLP.forward(x)\n",
    "        x_flatten = x.view(x.size(0), -1) #need to flatten batch_size x 1 x 28 x 28 to batch_size x 28*28\n",
    "        out = self.hidden_layers(x_flatten)\n",
    "        out = self.hidden_to_output(out) #you can redefine variables\n",
    "        return self.logsoftmax(out)\n",
    "        \n",
    "mlp = MLP(num_layers = 1)\n",
    "print(mlp)\n",
    "for p in mlp.parameters(): #all the parameters that were defined inside the module can be accessed like this\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the network is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = mlp(Variable(image))\n",
    "print(y_pred) #these will be log probabilities over each of the 10 classes\n",
    "print(y_pred[0].exp()) #let's make sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also convenient to define a test function that we can call periodically to check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation performance. NLL: 2.3212, Accuracy: 0.1380\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss() #this is the negative log-likelihood for multi-class classification\n",
    "\n",
    "def test(model, data):\n",
    "    correct = 0.\n",
    "    num_examples = 0.\n",
    "    nll = 0.\n",
    "    for (image, label) in data:\n",
    "        image, label = Variable(image), Variable(label) #annoying, but necessary        \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)\n",
    "        nll += nll_batch.data[0] * image.size(0) #by default NLL is averaged over each batch\n",
    "        y_pred_max, y_pred_argmax = torch.max(y_pred, 1) #prediction is the argmax\n",
    "        correct += (y_pred_argmax.data == label.data).sum() \n",
    "        num_examples += image.size(0) \n",
    "    return nll/num_examples, correct/num_examples\n",
    "nll, accuracy = test(mlp, val_loader)\n",
    "print('Validation performance. NLL: %.4f, Accuracy: %.4f'% (nll, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr =0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for (image, label) in train_loader:\n",
    "        optim.zero_grad()\n",
    "        image, label = Variable(image), Variable(label) \n",
    "        #y_pred = mlp(image)\n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test(mlp, train_loader) #you never wanna do this in practice, since this will take forever\n",
    "    nll_val, accuracy_val = test(mlp, val_loader)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with more hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MLP(num_layers = 1, hidden_dim = 100)\n",
    "print(mlp)\n",
    "optim = torch.optim.SGD(mlp.parameters(), lr = 0.5)\n",
    "num_epochs = 20\n",
    "for e in range(num_epochs):\n",
    "    for (image, label) in train_loader:\n",
    "        optim.zero_grad()\n",
    "        image, label = Variable(image), Variable(label) \n",
    "        y_pred = mlp(image)\n",
    "        nll_batch = criterion(y_pred, label)    \n",
    "        nll_batch.backward()\n",
    "        optim.step()\n",
    "    nll_train, accuracy_train = test(mlp, train_loader) \n",
    "    nll_val, accuracy_val = test(mlp, val_loader)\n",
    "    print('Training performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_train, accuracy_train))\n",
    "    print('Validation performance after epoch %d: NLL: %.4f, Accuracy: %.4f'% (e+1, nll_val, accuracy_val))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. For the rest of section, try implementing a ConvNet. You may need to use more sophisticated optimization algorithms (`optim.Adam(model.parameters(), lr = 0.001)` should work well enough for most models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convNet, self).__init__()     \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)        \n",
    "        \n",
    "    def forward(self, x): #MLP(x) is shorthand for MLP.forward(x)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "convNet = convNet()\n",
    "print(convNet)\n",
    "for p in convNet.parameters(): #all the parameters that were defined inside the module can be accessed like this\n",
    "    print(p.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = convNet(Variable(image))\n",
    "print(y_pred) #these will be log probabilities over each of the 10 classes\n",
    "print(y_pred[0].exp()) #let's make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Apple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
