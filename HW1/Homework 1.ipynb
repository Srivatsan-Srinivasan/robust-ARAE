{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be building several varieties of text classifiers.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in PyTorch:\n",
    "\n",
    "1. A naive Bayes unigram classifer (follow Wang and Manning http://www.aclweb.org/anthology/P/P12/P12-2.pdf#page=118: you should only implement Naive Bayes, not the combined classifer with SVM).\n",
    "2. A logistic regression model over word types (you can implement this as $y = \\sigma(\\sum_i W x_i + b)$) \n",
    "3. A continuous bag-of-word neural network with embeddings (similar to CBOW in Mikolov et al https://arxiv.org/pdf/1301.3781.pdf).\n",
    "4. A simple convolutional neural network (any variant of CNN as described in Kim http://aclweb.org/anthology/D/D14/D14-1181.pdf).\n",
    "5. Your own extensions to these models...\n",
    "\n",
    "Consult the papers provided for hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable(array, requires_grad=False):\n",
    "    if isinstance(array, np.ndarray):\n",
    "        return t.autograd.Variable(t.from_numpy(array), requires_grad=requires_grad)\n",
    "    elif isinstance(array, list) or isinstance(array,tuple):\n",
    "        return t.autograd.Variable(t.from_numpy(np.array(array)), requires_grad=requires_grad)\n",
    "    elif isinstance(array, float) or isinstance(array, int):\n",
    "        return t.autograd.Variable(t.from_numpy(np.array([array])), requires_grad=requires_grad)\n",
    "    elif isinstance(array, t.Tensor):\n",
    "        return t.autograd.Variable(array, requires_grad=requires_grad)\n",
    "    else: raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use of this problem is known as the Stanford Sentiment Treebank (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf). It is a variant of a standard sentiment classification task. For simplicity, we will use the most basic form. Classifying a sentence as positive or negative in sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we input our data. Here we will use the standard SST train split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this data. It's still in its original form, we can see that each example consists of a label and the original words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 6920\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train_dataset))\n",
    "print('vars(train[0])', vars(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map this data to features, we need to assign an index to each word an label. The function build vocab allows us to do this and provides useful options that we will need in future assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 16286\n",
      "len(LABEL.vocab) 3\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_dataset)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to create batches of our training data that can be used for training and validating the model. This function produces 3 iterators that will let us go through the train, val and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train_dataset, val_dataset, test_dataset), batch_size=10, device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a single batch from one of these iterators. The library automatically converts the underlying words into indices. It then produces tensors for batches of x and y. In this case it will consist of the number of words of the longest sentence (with padding) followed by the number of batches. We can use the vocabulary dictionary to convert back from these indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch [max sent length, batch size] torch.Size([24, 10])\n",
      "Second in batch Variable containing:\n",
      "  2128\n",
      "    12\n",
      "    22\n",
      "  3496\n",
      "   239\n",
      " 15875\n",
      "  3451\n",
      "  1642\n",
      "     6\n",
      "  3370\n",
      "   129\n",
      "     3\n",
      "  4798\n",
      "    12\n",
      "    22\n",
      "   488\n",
      "   871\n",
      "   809\n",
      "     5\n",
      "  2813\n",
      "    12\n",
      "    22\n",
      "   536\n",
      "     2\n",
      "[torch.LongTensor of size 24]\n",
      "\n",
      "Converted back to string:  Warm in its loving yet unforgivingly inconsistent depiction of everyday people , relaxed in its perfect quiet pace and proud in its message .\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Size of text batch [max sent length, batch size]\", batch.text.size())\n",
    "print(\"Second in batch\", batch.text[:, 0])\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 0].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly it produces a vector for each of the labels in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of label batch [batch size] torch.Size([10])\n",
      "Second in batch Variable containing:\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Converted back to string:  positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of label batch [batch size]\", batch.label.size())\n",
    "print(\"Second in batch\", batch.label[0])\n",
    "print(\"Converted back to string: \", LABEL.vocab.itos[batch.label.data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the Vocab object can be used to map pretrained word vectors to the indices in the vocabulary. This will be very useful for part 3 and 4 of the problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Build the vocabulary with word embeddings\n",
    "# url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "# TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))\n",
    "\n",
    "# print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "# print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\wiki.simple.vec: 293MB [00:42, 6.90MB/s]                                                                 \n",
      "  0%|                                                                                       | 0/111052 [00:00<?, ?it/s]Skipping token 111051 with 1-dimensional vector ['300']; likely a header\n",
      "100%|████████████████████████████████████████████████████████████████████████| 111052/111052 [00:21<00:00, 5091.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "TEXT2 = deepcopy(TEXT)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT2.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings size  torch.Size([16286, 300])\n",
      "Word embedding of 'follows', first 10 dim  \n",
      " 0.2057\n",
      " 0.1047\n",
      "-0.3900\n",
      "-0.1086\n",
      "-0.0722\n",
      "-0.1184\n",
      "-0.1109\n",
      " 0.1917\n",
      " 0.4781\n",
      " 2.0576\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "TEXT.vocab.load_vectors(vectors=GloVe())\n",
    "\n",
    "print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])# Build the vocabulary with word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 4 different torch models that take in batch.text and produce a distribution over labels. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition:  https://www.kaggle.com/c/harvard-cs281-hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \"All models should be able to be run with following command.\"\n",
    "    upload = []\n",
    "    # Update: for kaggle the bucket iterator needs to have batch_size 10\n",
    "#     test_iter = torchtext.data.BucketIterator(test_dataset, train=False, batch_size=10)\n",
    "    for batch in test_iter:\n",
    "        # Your prediction data here (don't cheat!)\n",
    "        probs = NB(batch.text).long()\n",
    "        upload += list(probs.data)\n",
    "\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        for u in upload:\n",
    "            f.write(str(u) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you should put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model: NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_sentence(batch, vec_dim=300, sentence_length=16):\n",
    "    \"\"\"Convert integer-encoded sentence to word vector representation\"\"\"\n",
    "    return t.cat([TEXT.vocab.vectors[batch.text.data.long()[:,i]].view(1,sentence_length,vec_dim) for i in range(batch.batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the NB weights. Then you classify with t.sign(t.sum(W(text)) + bias)\n",
    "# You can do that because the features are the indicators variables of the word occurences\n",
    "W = t.nn.Embedding(len(TEXT.vocab), 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter.batch_size = 10\n",
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████▊    | 655/692 [00:01<00:00, 543.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "i = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "# count the occurences of each word (classwise)\n",
    "for b in tqdm(train_iter):\n",
    "    i += 1\n",
    "    pos_tmp = t.nonzero((b.label==1).data.long()).numpy().flatten().shape[0]\n",
    "    neg_tmp = t.nonzero((b.label==2).data.long()).numpy().flatten().shape[0]\n",
    "    pos += pos_tmp\n",
    "    neg += neg_tmp\n",
    "    if neg_tmp < 10:\n",
    "        positive_counts += Counter(b.text.transpose(0,1).index_select(0, t.nonzero((b.label==1).data.long()).squeeze()).data.numpy().flatten().tolist())\n",
    "    if pos_tmp < 10:\n",
    "        negative_counts += Counter(b.text.transpose(0,1).index_select(0, t.nonzero((b.label==2).data.long()).squeeze()).data.numpy().flatten().tolist())\n",
    "    if i >= 692:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(len(TEXT.vocab)):  # pseudo counts\n",
    "    positive_counts[k] += 1\n",
    "    negative_counts[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale_pos = sum(list(positive_counts.values()))\n",
    "scale_neg = sum(list(negative_counts.values()))\n",
    "positive_prop = {k: v/scale_pos for k,v in positive_counts.items()}\n",
    "negative_prop = {k: v/scale_neg for k,v in negative_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = {k: np.log(positive_prop[k] / negative_prop[k]) for k in range(len(TEXT.vocab))}\n",
    "W.weight.data = t.from_numpy(np.array([r[k] for k in range(len(TEXT.vocab))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias = np.log(pos/neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB(text):\n",
    "    \"\"\"sign(Wx + b)\"\"\"\n",
    "    return t.sign(t.cat([t.sum(W(text.transpose(0,1)[i])) for i in range(text.data.numpy().shape[1])]) + bias).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "upload = []\n",
    "true = []\n",
    "for batch in test_iter:\n",
    "    # Your prediction data here (don't cheat!)\n",
    "    probs = NB(batch.text).long()\n",
    "    upload += list(probs.data)\n",
    "    true += batch.label.data.numpy().tolist()\n",
    "true = [x if x == 1 else -1 for x in true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8237232289950577"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([(x*y == 1) for x,y in zip(upload,true)])/ len(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = t.nn.Embedding(len(TEXT.vocab), 1)\n",
    "b = variable(0., True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "nll = t.nn.NLLLoss(size_average=True)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = t.optim.RMSprop([b, W.weight], lr=learning_rate)\n",
    "sig = t.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 10)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter), train_iter.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def eval_perf(iterator):\n",
    "    count = 0\n",
    "    bs = iterator.batch_size * 1\n",
    "    iterator.batch_size = 1\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # get data\n",
    "        y_pred = (sig(t.cat([W(batch.text.transpose(0,1)[i]).sum() for i in range(batch.text.data.numpy().shape[1])]) + b.float()) > 0.5).long()\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "\n",
    "        count += t.sum((y == y_pred).long())\n",
    "        if i >= len(iterator) - 1:\n",
    "            break\n",
    "    iterator.batch_size = bs\n",
    "    return (count.float() / len(iterator)).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy after 0 epochs: 0.65\n",
      "Validation accuracy after 1 epochs: 0.70\n",
      "Validation accuracy after 2 epochs: 0.73\n",
      "Validation accuracy after 3 epochs: 0.73\n",
      "Validation accuracy after 4 epochs: 0.74\n",
      "Validation accuracy after 5 epochs: 0.74\n",
      "Validation accuracy after 6 epochs: 0.75\n",
      "Validation accuracy after 7 epochs: 0.76\n",
      "Validation accuracy after 8 epochs: 0.77\n",
      "Validation accuracy after 9 epochs: 0.77\n",
      "Validation accuracy after 10 epochs: 0.78\n",
      "Validation accuracy after 11 epochs: 0.78\n",
      "Validation accuracy after 12 epochs: 0.78\n",
      "Validation accuracy after 13 epochs: 0.78\n",
      "Validation accuracy after 14 epochs: 0.78\n",
      "Validation accuracy after 15 epochs: 0.78\n",
      "Validation accuracy after 16 epochs: 0.78\n",
      "Validation accuracy after 17 epochs: 0.77\n",
      "Validation accuracy after 18 epochs: 0.77\n",
      "Validation accuracy after 19 epochs: 0.78\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "n_epochs = 20\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # get data\n",
    "        y_pred = sig(t.cat([W(batch.text.transpose(0,1)[i]).sum() for i in range(batch.text.data.numpy().shape[1])]) + b.float()).unsqueeze(1)\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "        \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss\n",
    "        y_pred = t.cat([1-y_pred, y_pred], 1).float()  # nll needs two inputs: the prediction for the negative/positive classes\n",
    "\n",
    "        loss = nll.forward(y_pred, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "        if i >= len(train_iter) - 1:\n",
    "            break\n",
    "    train_iter.init_epoch()\n",
    "    print(\"Validation accuracy after %d epochs: %.2f\" % (_, eval_perf(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(text, vdim=300):\n",
    "    length, batch_size = text.data.numpy().shape\n",
    "    return t.mean(t.cat([TEXT.vocab.vectors[text.long().data.transpose(0,1)[i]].view(1,length,vdim) for i in range(batch_size)]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = variable(np.random.normal(0, .1, (300,)), True)\n",
    "b = variable(0., True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "nll = t.nn.NLLLoss(size_average=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = t.optim.RMSprop([b, W], lr=learning_rate)\n",
    "sig = t.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def eval_perf(iterator):\n",
    "    count = 0\n",
    "    bs = iterator.batch_size * 1\n",
    "    iterator.batch_size = 1\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # get data\n",
    "        text_ = batch.text\n",
    "        length = text_.data.numpy().shape[0]\n",
    "        y_pred = (sig(t.mm(variable(vectorize(text_)),W.float().resize(300,1)).squeeze() + b.float().squeeze()) > 0.5).long()\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "\n",
    "        count += t.sum((y == y_pred).long())\n",
    "        if i >= len(iterator) - 1:\n",
    "            break\n",
    "    iterator.batch_size = bs\n",
    "    return (count.float() / len(iterator)).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy after 0 epochs: 0.71\n",
      "Validation accuracy after 1 epochs: 0.70\n",
      "Validation accuracy after 2 epochs: 0.70\n",
      "Validation accuracy after 3 epochs: 0.71\n",
      "Validation accuracy after 4 epochs: 0.72\n",
      "Validation accuracy after 5 epochs: 0.71\n",
      "Validation accuracy after 6 epochs: 0.69\n",
      "Validation accuracy after 7 epochs: 0.71\n",
      "Validation accuracy after 8 epochs: 0.71\n",
      "Validation accuracy after 9 epochs: 0.71\n",
      "Validation accuracy after 10 epochs: 0.71\n",
      "Validation accuracy after 11 epochs: 0.71\n",
      "Validation accuracy after 12 epochs: 0.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-254-2ab122ca28b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 98\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "n_epochs = 20\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # get data\n",
    "        text_ = batch.text\n",
    "        length = text_.data.numpy().shape[0]\n",
    "        y_pred = sig(t.mm(variable(vectorize(text_)),W.float().resize(300,1)).squeeze() + b.float().squeeze()).unsqueeze(1)\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "        \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss\n",
    "        y_pred = t.cat([1-y_pred, y_pred], 1).float()  # nll needs two inputs: the prediction for the negative/positive classes\n",
    "\n",
    "        loss = nll.forward(y_pred, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "        if i >= len(train_iter) - 1:\n",
    "            break\n",
    "    train_iter.init_epoch()\n",
    "    print(\"Validation accuracy after %d epochs: %.2f\" % (_, eval_perf(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model 4: convnet on word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Conv1d as conv, MaxPool1d as maxpool, Linear as fc, Softmax, ReLU, Dropout, Tanh, BatchNorm1d as BN, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax = Softmax()\n",
    "dropout = Dropout()\n",
    "relu = ReLU()\n",
    "tanh = Tanh()\n",
    "lrelu = LeakyReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best perf with:\n",
    "* Adam, lr 0.001\n",
    "* conv 50 filters, padding 1, kernel 3\n",
    "* one FC layer\n",
    "* batch size 100\n",
    "* dropout 25% just before the FC layer\n",
    "* relu activation\n",
    "* GloVe 840B embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convnet(t.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Convnet, self).__init__()\n",
    "        self.conv1 = conv(300, 100, 3, padding=1)\n",
    "        self.dropout1 = Dropout(.25)\n",
    "#         self.dropout2 = Dropout(.1)\n",
    "        self.fc2 = fc(100, 20)\n",
    "        self.fc3 = fc(20, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xx = lrelu(self.conv1(x))\n",
    "        xx = t.max(xx, -1)[0]\n",
    "        xx = self.dropout1(xx)\n",
    "        xx = lrelu(self.fc2(xx))\n",
    "#         xx = self.dropout2(xx)\n",
    "        xx = self.fc3(xx)\n",
    "        return softmax(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HRFConvnet(t.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HRFConvnet, self).__init__()\n",
    "        self.conv1a = conv(300, 25, 3, padding=1)\n",
    "        self.conv1b = conv(300, 25, 5, padding=2)\n",
    "        self.conv2 = conv(100, 50, 2, padding=1)\n",
    "        self.maxpool = t.nn.MaxPool1d(3, padding=1)\n",
    "        self.avgpool = t.nn.AvgPool1d(3, padding=1)\n",
    "        self.dropout = Dropout(.25)\n",
    "        self.fc2 = fc(200, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolutions and pooling\n",
    "        xx = lrelu(t.cat([self.conv1a(x), self.conv1b(x)], 1))\n",
    "        xx = t.cat([self.maxpool(xx), self.avgpool(xx)], 1)\n",
    "        xx = lrelu(self.conv2(xx))\n",
    "        \n",
    "        # several kinds of pooling over time\n",
    "        xx_max = t.max(xx, -1)[0]\n",
    "        xx_mean = t.mean(xx, -1)\n",
    "        xx_min = t.min(xx, -1)[0]\n",
    "        xx_med = t.median(xx, -1)[0]\n",
    "        xx = t.cat([xx_max, xx_mean, xx_min, xx_med], -1)\n",
    "        \n",
    "        # dropout and linear layer\n",
    "        xx = self.dropout(xx)\n",
    "        xx = self.fc2(xx)\n",
    "        return softmax(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SHRFConvnet(t.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SHRFConvnet, self).__init__()\n",
    "        self.conv1a = conv(300, 25, 3, padding=1)\n",
    "        self.conv1b = conv(300, 25, 5, padding=2)\n",
    "        self.conv2 = conv(50, 50, 2, padding=1)\n",
    "        self.maxpool = t.nn.MaxPool1d(3, padding=1)\n",
    "        self.dropout = Dropout(.25)\n",
    "        self.fc2 = fc(50, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # convolutions and pooling\n",
    "        xx = lrelu(t.cat([self.conv1a(x), self.conv1b(x)], 1))\n",
    "        xx = self.maxpool(xx)\n",
    "        xx = lrelu(self.conv2(xx))\n",
    "        \n",
    "        # several kinds of pooling over time\n",
    "        xx = t.max(xx, -1)[0]\n",
    "        \n",
    "        # dropout and linear layer\n",
    "        xx = self.dropout(xx)\n",
    "        xx = self.fc2(xx)\n",
    "        return softmax(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "nll = t.nn.NLLLoss(size_average=True)\n",
    "learning_rate = 1e-3\n",
    "convnet = Convnet()\n",
    "optimizer = t.optim.Adam(convnet.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(text, vdim=300):\n",
    "    length, batch_size = text.data.numpy().shape\n",
    "    return t.cat([TEXT.vocab.vectors[text.long().data.transpose(0,1)[i]].view(1,length,vdim) for i in range(batch_size)]).permute(0,2,1)\n",
    "\n",
    "def vectorize2(text, vdim=300):\n",
    "    length, batch_size = text.data.numpy().shape\n",
    "    x = t.cat([TEXT.vocab.vectors[text.long().data.transpose(0,1)[i]].view(1,length,vdim) for i in range(batch_size)]).permute(0,2,1)\n",
    "    return t.cat([x, t.cat([TEXT2.vocab.vectors[text.long().data.transpose(0,1)[i]].view(1,length,vdim) for i in range(batch_size)]).permute(0,2,1)], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def eval_perf(iterator):\n",
    "    count = 0\n",
    "    bs = iterator.batch_size * 1\n",
    "    iterator.batch_size = 1\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # get data\n",
    "        text = batch.text\n",
    "        y_pred = (convnet(variable(vectorize(text)))[:, 1] > 0.5).long()\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "\n",
    "        count += t.sum((y == y_pred).long())\n",
    "        if i >= len(iterator) - 1:\n",
    "            break\n",
    "    iterator.batch_size = bs\n",
    "    return (count.float() / (bs*len(iterator))).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy after 0 epochs: 0.829\n",
      "Validation accuracy after 1 epochs: 0.818\n",
      "Validation accuracy after 2 epochs: 0.828\n",
      "Validation accuracy after 3 epochs: 0.818\n",
      "Validation accuracy after 4 epochs: 0.812\n",
      "Validation accuracy after 5 epochs: 0.819\n",
      "Validation accuracy after 6 epochs: 0.826\n",
      "Validation accuracy after 7 epochs: 0.822\n",
      "Validation accuracy after 8 epochs: 0.821\n",
      "Validation accuracy after 9 epochs: 0.820\n",
      "Validation accuracy after 10 epochs: 0.835\n",
      "Validation accuracy after 11 epochs: 0.815\n",
      "Validation accuracy after 12 epochs: 0.808\n",
      "Validation accuracy after 13 epochs: 0.825\n",
      "Validation accuracy after 14 epochs: 0.821\n",
      "Validation accuracy after 15 epochs: 0.815\n",
      "Validation accuracy after 16 epochs: 0.821\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-671-27e6a17241a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# get data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-658-b93f35efb1dc>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mxx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 154\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[1;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[0;32m     81\u001b[0m     f = ConvNd(_single(stride), _single(padding), _single(dilation), False,\n\u001b[0;32m     82\u001b[0m                _single(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "n_epochs = 25\n",
    "\n",
    "train_iter.batch_size = 100\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # get data\n",
    "        text = batch.text\n",
    "        y_pred = convnet(variable(vectorize(text)))\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "        \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss\n",
    "        loss = nll.forward(y_pred, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "    train_iter.init_epoch()\n",
    "    convnet.eval()\n",
    "    val_perf = eval_perf(val_iter)\n",
    "    print(\"Validation accuracy after %d epochs: %.3f\" % (_, val_perf))\n",
    "    if val_perf >= .835: break\n",
    "    convnet.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82841527"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet.eval()\n",
    "eval_perf(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(convnet(variable(vectorize(batch.text))).max(1)[1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \"All models should be able to be run with following command.\"\n",
    "    upload = []\n",
    "    # Update: for kaggle the bucket iterator needs to have batch_size 10\n",
    "    test_iter = torchtext.data.BucketIterator(test_dataset, train=False, batch_size=10)\n",
    "    for batch in test_iter:\n",
    "        # Your prediction data here (don't cheat!)\n",
    "        probs = model(variable(vectorize(b.text)))\n",
    "        _, argmax = probs.max(1)\n",
    "        upload += [x if x==1 else 2 for x in list(argmax.data)]\n",
    "    \n",
    "    id = 0\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        f.write(\"Id,Cat\" + \"\\n\")        \n",
    "        for u in upload:\n",
    "            f.write(str(id) + \",\"+ str(u) + \"\\n\")\n",
    "            id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  5585    296   1061  ...     437    457     29\n",
      "     3      7     44  ...      10  14952     10\n",
      "   284   7254      7  ...     294    218  10355\n",
      "        ...            ⋱           ...         \n",
      "     5    501      4  ...    3219    185    256\n",
      "  1511  15923   4642  ...       2      2      2\n",
      "     2      2      2  ...       1      1      1\n",
      "[torch.LongTensor of size 11x100]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.8744e-05  9.9998e-01\n",
       " 1.0000e+00  5.7803e-10\n",
       " 9.9182e-01  8.1791e-03\n",
       " 9.9839e-01  1.6110e-03\n",
       " 3.6838e-04  9.9963e-01\n",
       " 9.9467e-01  5.3323e-03\n",
       " 1.0000e+00  1.7794e-10\n",
       " 6.4967e-04  9.9935e-01\n",
       " 2.3464e-05  9.9998e-01\n",
       " 4.6551e-06  1.0000e+00\n",
       " 5.3114e-04  9.9947e-01\n",
       " 1.0709e-08  1.0000e+00\n",
       " 1.0000e+00  2.5872e-06\n",
       " 1.0000e+00  5.9545e-07\n",
       " 9.9917e-01  8.3254e-04\n",
       " 9.9973e-01  2.6982e-04\n",
       " 1.0000e+00  1.3791e-10\n",
       " 9.9816e-01  1.8364e-03\n",
       " 1.0831e-07  1.0000e+00\n",
       " 3.9618e-03  9.9604e-01\n",
       " 9.9911e-01  8.8839e-04\n",
       " 9.5423e-06  9.9999e-01\n",
       " 5.1716e-01  4.8284e-01\n",
       " 1.1291e-04  9.9989e-01\n",
       " 8.7021e-04  9.9913e-01\n",
       " 9.9719e-01  2.8053e-03\n",
       " 9.9240e-01  7.6007e-03\n",
       " 9.9529e-01  4.7131e-03\n",
       " 1.1589e-04  9.9988e-01\n",
       " 7.8056e-03  9.9219e-01\n",
       " 1.1179e-07  1.0000e+00\n",
       " 9.8973e-01  1.0271e-02\n",
       " 7.7427e-07  1.0000e+00\n",
       " 2.7379e-06  1.0000e+00\n",
       " 1.0000e+00  5.6208e-09\n",
       " 9.9998e-01  2.4375e-05\n",
       " 2.1724e-04  9.9978e-01\n",
       " 1.0000e+00  1.1609e-07\n",
       " 1.0000e+00  8.0384e-07\n",
       " 9.9991e-01  8.8074e-05\n",
       " 3.7483e-03  9.9625e-01\n",
       " 6.1051e-03  9.9389e-01\n",
       " 1.0490e-08  1.0000e+00\n",
       " 4.7905e-08  1.0000e+00\n",
       " 9.9999e-01  7.3596e-06\n",
       " 4.5836e-06  1.0000e+00\n",
       " 1.5031e-02  9.8497e-01\n",
       " 1.2800e-03  9.9872e-01\n",
       " 3.5706e-01  6.4294e-01\n",
       " 2.6388e-03  9.9736e-01\n",
       " 9.9904e-01  9.5522e-04\n",
       " 3.9765e-03  9.9602e-01\n",
       " 9.9999e-01  5.0815e-06\n",
       " 9.8871e-01  1.1290e-02\n",
       " 1.0000e+00  4.9742e-06\n",
       " 9.9973e-01  2.7209e-04\n",
       " 9.9236e-01  7.6369e-03\n",
       " 5.0620e-04  9.9949e-01\n",
       " 1.0000e+00  4.7853e-08\n",
       " 1.0000e+00  1.2822e-08\n",
       " 1.0000e+00  2.7537e-06\n",
       " 9.9667e-01  3.3264e-03\n",
       " 1.0000e+00  8.8883e-09\n",
       " 3.8275e-04  9.9962e-01\n",
       " 9.9999e-01  5.1136e-06\n",
       " 1.6725e-05  9.9998e-01\n",
       " 9.8853e-01  1.1470e-02\n",
       " 1.6520e-08  1.0000e+00\n",
       " 2.0001e-04  9.9980e-01\n",
       " 2.9554e-09  1.0000e+00\n",
       " 3.9220e-09  1.0000e+00\n",
       " 9.7744e-01  2.2558e-02\n",
       " 9.9865e-01  1.3471e-03\n",
       " 9.2817e-03  9.9072e-01\n",
       " 6.8338e-02  9.3166e-01\n",
       " 1.0000e+00  2.1191e-07\n",
       " 9.7996e-01  2.0040e-02\n",
       " 7.9160e-05  9.9992e-01\n",
       " 9.9982e-01  1.7786e-04\n",
       " 2.1488e-04  9.9979e-01\n",
       " 9.9146e-01  8.5421e-03\n",
       " 9.9468e-01  5.3178e-03\n",
       " 2.6360e-04  9.9974e-01\n",
       " 9.9996e-01  3.5123e-05\n",
       " 4.9178e-05  9.9995e-01\n",
       " 3.1284e-04  9.9969e-01\n",
       " 9.9962e-01  3.7660e-04\n",
       " 2.1187e-04  9.9979e-01\n",
       " 9.9076e-01  9.2394e-03\n",
       " 9.9926e-01  7.4249e-04\n",
       " 9.9999e-01  5.6044e-06\n",
       " 5.9183e-04  9.9941e-01\n",
       " 9.9992e-01  8.4105e-05\n",
       " 2.1866e-03  9.9781e-01\n",
       " 1.0000e+00  1.9285e-07\n",
       " 1.0000e+00  4.0993e-09\n",
       " 3.5626e-07  1.0000e+00\n",
       " 5.1785e-05  9.9995e-01\n",
       " 9.9546e-01  4.5447e-03\n",
       " 9.9999e-01  8.6467e-06\n",
       "[torch.FloatTensor of size 100x2]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(batch.text)\n",
    "convnet.eval()\n",
    "convnet(variable(vectorize(batch.text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have a look at specific mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch2text(batch):\n",
    "    return '\\n'.join([\" \".join([TEXT.vocab.itos[i] for i in batch.text[:, j].data]) for j in range(batch.text.data.numpy().shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "true = []\n",
    "pred = []\n",
    "convnet.eval()\n",
    "\n",
    "for batch in val_iter:\n",
    "    proba_pred = convnet(variable(vectorize(batch.text)))[:, 1]\n",
    "    label_pred = (proba_pred > 0.5).long()\n",
    "    y = batch.label.long()*(-1) + 2\n",
    "    for yyp, yyp_, yy, text in zip(label_pred, proba_pred, y, batch2text(batch).split('\\n')):\n",
    "        if yy.data.numpy()[0] != yyp.data.numpy()[0]:\n",
    "            true.append(yy.data.numpy()[0])\n",
    "            pred.append(yyp_.data.numpy()[0])\n",
    "            errors.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 880, 0.1715909090909091)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(errors), len(val_iter)*val_iter.batch_size, len(errors) / (len(val_iter)*val_iter.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3.53044e-05 Funny but <unk> slight .\n",
      "0 0.747016 Oh come on . <pad>\n",
      "1 0.00295216 Cool ? <pad> <pad> <pad>\n",
      "1 9.97105e-09 <unk> inept and ridiculous .\n",
      "1 0.000109407 As unseemly as its title suggests .\n",
      "1 0.000464429 Good film , but very glum .\n",
      "1 0.166885 But it still <unk> in the pocket .\n",
      "1 0.0098425 A painfully funny ode to bad behavior .\n",
      "1 0.0211383 My thoughts were focused on the characters .\n",
      "1 0.000118771 <unk> potentially forgettable formula into something strangely diverting .\n",
      "0 0.999748 Rarely has <unk> looked so shimmering and <unk> .\n",
      "0 0.996592 It 's not the ultimate <unk> gangster movie .\n",
      "0 0.980605 An absurdist comedy about alienation , separation and loss .\n",
      "1 0.000176797 <unk> neatly into the category of Good Stupid Fun .\n",
      "0 0.981386 An <unk> amalgam of <unk> News and <unk> . <pad>\n",
      "1 0.0269448 ... routine , harmless diversion and little else . <pad>\n",
      "1 0.0420371 Despite its title , Punch-Drunk Love is never heavy-handed .\n",
      "1 0.3446 A woman 's pic directed with resonance by <unk> Chaiken .\n",
      "1 0.0485616 Fun , <unk> and terribly hip bit of cinematic entertainment .\n",
      "0 0.687034 Or doing last year 's <unk> with your <unk> . <pad>\n",
      "0 0.51938 The <unk> story needs more dramatic meat on its bones .\n",
      "1 0.00422453 Instead , he shows them the respect they are due .\n",
      "0 0.99941 Burns never really <unk> to full effect the energetic cast . <pad>\n",
      "0 0.811823 <unk> , profane , packed with cartoonish violence and <unk> characters .\n",
      "0 0.997976 The lower your expectations , the more you 'll enjoy it .\n",
      "1 0.215477 The jabs it employs are short , carefully placed and <unk> .\n",
      "0 0.823437 But this films lacks the passion required to sell the material .\n",
      "0 0.99769 All that 's missing is the spontaneity , originality and delight .\n",
      "1 0.327567 Minority Report is exactly what the title <unk> , a report .\n",
      "1 0.281485 Puts a human face on a land most Westerners are <unk> with .\n",
      "1 0.000154649 We have n't seen such hilarity since <unk> It Is n't So !\n",
      "0 0.540028 Looks and feels like a project better suited for the small screen .\n",
      "1 0.327911 And that 's a big part of why we go to the movies .\n",
      "0 0.999999 American Chai encourages rueful laughter at stereotypes only an <unk> would recognize . <pad>\n",
      "0 0.928365 Movie fans , get ready to take off ... the other direction . <pad>\n",
      "1 0.0151384 So much facile technique , such cute ideas , so little movie . <pad>\n",
      "0 0.925522 Comes ... uncomfortably close to coasting in the treads of The <unk> <unk> .\n",
      "1 0.288523 It 's about following your dreams , no matter what your parents think .\n",
      "0 0.861361 It 's one <unk> world when even <unk> <unk> around group therapy sessions .\n",
      "0 0.999802 <unk> the value of its <unk> of archival <unk> with its <unk> <unk> .\n",
      "0 0.990573 <unk> 's determination to <unk> you in sheer , unrelenting <unk> is <unk> .\n",
      "0 0.999987 It offers little beyond the <unk> joys of pretty and weightless intellectual entertainment . <pad>\n",
      "1 0.00111779 Though only 60 minutes long , the film is packed with information and impressions .\n",
      "0 0.981716 Moretti 's compelling <unk> of grief and the difficult process of <unk> to loss .\n",
      "1 0.189922 Manages to transcend the sex , drugs and <unk> plot into something far richer .\n",
      "0 0.999999 A great ensemble cast ca n't lift this heartfelt enterprise out of the familiar .\n",
      "0 1.0 <unk> buffs might love this film , but others will find its pleasures <unk> .\n",
      "0 0.969174 A better title , for all concerned , might be Swept Under the <unk> .\n",
      "0 0.872127 I 'll bet the video game is a lot more fun than the film .\n",
      "1 0.0384252 Scooby <unk> Doo \\/ And Shaggy too \\/ You both look and sound great . <pad>\n",
      "1 0.236672 It 's hard to imagine Alan <unk> being better than he is in this performance .\n",
      "1 2.16956e-05 No screen <unk> in recent memory has the <unk> of Clones ' last <unk> minutes .\n",
      "1 0.000136972 If Steven Soderbergh 's ` Solaris ' is a failure it is a glorious failure .\n",
      "1 0.112658 <unk> the studio , Piccoli is <unk> affecting and so is this <unk> minimalist movie .\n",
      "0 0.722733 Sam Mendes has become <unk> at the <unk> for <unk> <unk> and <unk> <unk> Out .\n",
      "0 0.999128 The script kicks in , and Mr. Hartley 's <unk> pace and <unk> rhythms follow .\n",
      "1 0.307618 Jaglom ... put -LRB- <unk> -RRB- the audience in the privileged position of <unk> on his characters\n",
      "0 0.65167 Another in-your-face <unk> in the lower depths made by people who have never sung those blues . <pad>\n",
      "0 0.869084 The only excitement comes when the credits finally roll and you get to leave the theater . <pad>\n",
      "0 0.563716 Every time you look , Sweet Home Alabama is taking another <unk> of a wrong turn . <pad>\n",
      "0 0.995493 Every dance becomes about <unk> , where <unk> and <unk> are celebrated , and sex is <unk> .\n",
      "1 0.0884124 Mr. Tsai is a very original artist in his medium , and What Time Is It There ?\n",
      "0 0.871606 In execution , this clever idea is far less funny than the original , <unk> From Space .\n",
      "1 0.0242005 The draw -LRB- for `` Big Bad Love '' -RRB- is a solid performance by Arliss Howard .\n",
      "0 0.983697 The director knows how to apply <unk> gloss , but his portrait of <unk> is strictly sitcom . <pad>\n",
      "1 0.000123416 Light years \\/ several <unk> speeds \\/ levels and levels of <unk> <unk> better than the pitiful <unk> .\n",
      "1 0.0408665 ` De Niro ... is a <unk> source of sincere passion that this Hollywood contrivance orbits around . '\n",
      "1 0.0449244 Not since Tom <unk> in <unk> <unk> has an actor made such a strong impression in his underwear .\n",
      "0 0.999632 Delivers the same old same old , <unk> up with Latin <unk> and turned out by Hollywood <unk> .\n",
      "0 0.940797 Chabrol has taken promising material for a black comedy and turned it instead into a somber <unk> drama .\n",
      "0 0.974415 Late Marriage 's <unk> is unlikely to demonstrate the emotional clout to sweep U.S. viewers off their feet . <pad>\n",
      "0 0.937576 It made me want to <unk> my eyes out of my head and toss them at the screen . <pad>\n",
      "0 1.0 In all , this is a watchable movie that 's not quite the memorable experience it might have been .\n",
      "1 0.000140391 A working class `` us vs. them '' opera that leaves no <unk> <unk> and no liberal cause <unk> .\n",
      "0 0.977689 With virtually no interesting elements for an audience to focus on , Chelsea Walls is a <unk> endurance challenge .\n",
      "0 0.893609 Every <unk> of the The New Guy reminds you that you could be doing something else far more pleasurable . <pad>\n",
      "0 0.899738 <unk> and miss as far as the comedy goes and a big <unk> ' miss in the way of story . <pad>\n",
      "0 0.999999 <unk> to be fun , and bouncy , with energetic musicals , the humor did n't quite engage this adult . <pad>\n",
      "0 0.967254 Although Huppert 's intensity and focus has a raw <unk> about it , The Piano Teacher is anything but fun . <pad>\n",
      "1 0.000257992 A full world has been presented onscreen , not some series of carefully structured plot points building to a pat resolution .\n",
      "0 0.615438 Corpus <unk> -- while undeniably interesting -- wore out its welcome well before the end credits <unk> about <unk> minutes in .\n",
      "0 0.994664 I 've always dreamed of attending Cannes , but after seeing this film , it 's not that big a deal .\n",
      "0 0.965416 Vera 's technical prowess ends up selling his film short ; he <unk> over hard truths even as he uncovers them .\n",
      "1 0.174991 Something akin to a Japanese Alice <unk> the Looking Glass , except that it seems to take itself far more seriously . <pad>\n",
      "0 0.932718 Nelson 's brutally unsentimental approach ... sucks the humanity from the film , leaving behind an horrific but weirdly <unk> spectacle . <pad>\n",
      "1 0.00148156 It seems like I have been waiting my whole life for this movie and now I ca n't wait for the sequel .\n",
      "1 0.00270034 No sophomore slump for director Sam Mendes , who <unk> from Oscar winner to Oscar-winning potential with a smooth <unk> of hand .\n",
      "1 0.467428 It deserves to be seen by anyone with even a passing interest in the events <unk> the world beyond their own horizons .\n",
      "1 1.15003e-05 Though it 's become almost redundant to say so , major <unk> go to Leigh for actually casting people who look working-class .\n",
      "1 0.00367008 We root for -LRB- <unk> and Paul -RRB- , even like them , though perhaps it 's an emotion closer to pity .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2.4222e-05 And if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\n",
      "0 0.513526 The humor is n't as sharp , the effects not as innovative , nor the story as imaginative as in the original . <pad>\n",
      "1 0.460046 Morton uses her face and her body language to bring us Morvern 's soul , even though the character is almost completely deadpan .\n",
      "0 0.999723 The experience of going to a film festival is a rewarding one ; the <unk> of <unk> one through this movie is not .\n",
      "1 0.479145 Not since Japanese filmmaker <unk> <unk> 's <unk> have the <unk> of combat and the specter of death been <unk> with such operatic grandeur .\n",
      "1 0.0279909 That dogged good will of the parents and ` vain ' Jia 's <unk> of ego , make the film touching despite some <unk> .\n",
      "0 0.982987 I <unk> with the plight of these families , but the movie does n't do a very good job conveying the issue at hand .\n",
      "0 0.984277 ... <unk> to provide a mix of smiles and tears , `` Crossroads '' instead provokes a handful of unintentional <unk> and numerous <unk> . <pad>\n",
      "1 0.317499 The Lion King was a <unk> success when it was released eight years ago , but on Imax it seems better , not just bigger .\n",
      "1 0.054997 It 's so good that its relentless , polished wit can withstand not only inept school productions , but even Oliver Parker 's movie adaptation .\n",
      "0 0.999993 This riveting World War II moral suspense story deals with the shadow side of American culture : racial prejudice in its ugly and diverse forms .\n",
      "0 0.650228 <unk> through this one , and you wo n't need a magic watch to stop time ; your DVD player will do it for you .\n",
      "0 0.999548 The talented and clever Robert Rodriguez perhaps put a little too much heart into his first film and did n't <unk> enough for his second .\n",
      "0 0.640388 Visually rather stunning , but ultimately a <unk> bore , the true creativity would have been to hide Treasure Planet entirely and completely <unk> it .\n",
      "1 0.186143 The movie is n't just hilarious : It 's witty and inventive , too , and in <unk> , it is n't even all that dumb .\n",
      "0 0.997684 The best that can be said about the work here of Scottish director Ritchie ... is that he obviously does n't have his heart in it .\n",
      "1 0.162645 While its careful pace and seemingly opaque story may not satisfy every moviegoer 's appetite , the film 's final scene is <unk> , transparently moving .\n",
      "0 0.997352 -LRB- <unk> -RRB- <unk> long on amiable monkeys and worthy <unk> , Jane Goodall 's Wild <unk> is short on the thrills the <unk> medium demands .\n",
      "0 0.756759 For all its impressive craftsmanship , and despite an overbearing series of <unk> <unk> , Lily Chou-Chou never really builds up a head of emotional steam .\n",
      "0 0.514715 The moviegoing equivalent of going to a dinner party and being forced to watch the host and <unk> 's home video of their baby 's <unk> .\n",
      "1 0.000282481 Fresnadillo 's dark and <unk> images have a way of <unk> into your subconscious like the nightmare you had a week ago that wo n't go away .\n",
      "0 0.748241 If you believe any of this , I can make you a real deal on <unk> <unk> stock that will double in value a week from Friday .\n",
      "0 0.579079 The film is based on truth and yet there is something about it that feels <unk> , as if the real story starts just around the corner .\n",
      "1 2.10636e-07 Writer\\/director Joe <unk> 's <unk> crime drama is a manual of <unk> cliches , but it moves fast enough to cover its clunky dialogue and lapses in logic .\n",
      "0 0.810432 Irwin is a man with enough charisma and audacity to carry a dozen films , but this particular result is ultimately held back from being something greater . <pad>\n",
      "0 0.998557 As the latest bid in the <unk> franchise game , I Spy makes its big-screen entry with little of the nervy originality of its groundbreaking small-screen <unk> . <pad>\n",
      "1 0.12942 Two hours fly by -- opera 's a pleasure when you do n't have to endure <unk> -- and even a <unk> to the form comes away <unk> .\n",
      "1 0.00258736 An effectively creepy , <unk> -LRB- not <unk> -RRB- film from Japanese director <unk> Nakata , who takes the <unk> curse on chain <unk> and actually applies it .\n",
      "1 0.0218126 A coda in every sense , The Pinochet <unk> <unk> time between a <unk> account of the British court 's <unk> <unk> game and the <unk> 's <unk> survivors .\n",
      "1 0.0643867 You 'll gasp <unk> and laugh <unk> and possibly , watching the spectacle of a promising young lad treading desperately in a nasty sea , <unk> an <unk> tear .\n",
      "0 0.904922 It showcases Carvey 's talent for voices , but not nearly enough and not without <unk> every drop of one 's patience to get to the good stuff . <pad>\n",
      "1 0.0757803 While there 's something <unk> funny about <unk> Anthony Hopkins saying ` Get in the car , <unk> , ' this Jerry Bruckheimer production has little else to offer <pad>\n",
      "1 0.28922 <unk> you into a <unk> , volatile , <unk> of a situation that quickly <unk> out of control , while focusing on the what much more than the why .\n",
      "1 0.00655589 On the heels of The Ring comes a similarly morose and humorless horror movie that , although flawed , is to be commended for its straight-ahead approach to creepiness .\n",
      "1 0.261748 ... a story we have n't seen on the big screen before , and it 's a story that we as Americans , and human beings , should know .\n",
      "0 0.970919 <unk> still need to function according to some set of believable and comprehensible impulses , no matter how many drugs they do or how much artistic license Avary employs . <pad>\n",
      "0 0.531803 The <unk> - it 's - surreal <unk> -LRB- featuring the voices of <unk> Close , <unk> <unk> and <unk> <unk> -RRB- brings back memories of cheesy old Godzilla flicks .\n",
      "1 0.0332158 Miller is playing so free with emotions , and the fact that children are <unk> to fortune , that he makes the audience hostage to his <unk> affectation of seriousness .\n",
      "0 0.688745 Once -LRB- Kim -RRB- begins to <unk> the shock tactics and <unk> metaphors , you may decide it 's too high a price to pay for a shimmering picture postcard .\n",
      "0 1.0 Care deftly captures the wonder and menace of growing up , but he never really embraces the joy of Fuhrman 's destructive escapism or the <unk> found by his characters .\n",
      "1 0.00800306 ... Mafia , rap stars and <unk> rats butt their ugly heads in a <unk> of cinematic violence that gives brutal <unk> to an unlikely , but likable , hero . '\n",
      "1 0.000212728 -LRB- D -RRB- <unk> n't bother being as cloying or preachy as equivalent <unk> Christian movies -- maybe the filmmakers know that the likely audience will already be among the faithful .\n",
      "1 0.240357 Like Mike is a winner for kids , and no doubt a winner for Lil Bow Wow , who can now add movies to the list of things he does well .\n",
      "1 0.0555185 It moves quickly , <unk> , and without fuss ; it does n't give you time to reflect on the <unk> -- and the Cold War <unk> -- of its premise .\n",
      "0 1.0 The story and the friendship proceeds in such a way that you 're watching a soap opera rather than a chronicle of the ups and downs that <unk> lifelong friendships . <pad>\n",
      "0 0.974408 If you 've ever entertained the notion of doing what the title of this film implies , what Sex With Strangers actually shows may put you off the idea forever . <pad>\n",
      "0 0.829951 While the Resident Evil games may have set new standards for thrills , suspense , and gore for video games , the movie really only succeeds in the third of these .\n",
      "0 0.995976 Pumpkin takes an admirable look at the hypocrisy of political correctness , but it does so with such an uneven tone that you never know when humor ends and tragedy begins .\n",
      "0 0.891923 By getting myself wrapped up in the visuals and eccentricities of many of the characters , I found myself confused when it came time to get to the heart of the movie .\n",
      "0 0.994175 Try as I may , I ca n't think of a single good reason to see this movie , even though everyone in my group <unk> <unk> , ` <unk> you ! '\n",
      "0 0.998516 If the movie succeeds in <unk> a <unk> sense of ` there but for the grace of God , ' it is far too self-conscious to draw you deeply into its world .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.999978 Very special effects , brilliantly bold colors and heightened reality ca n't hide the giant <unk> ' <unk> in `` Stuart Little 2 `` : There 's just no story , folks .\n",
      "0 0.842675 Even the finest <unk> ca n't make a <unk> into anything more than a <unk> , and Robert De Niro ca n't make this movie anything more than a trashy cop buddy comedy . <pad>\n",
      "0 0.999426 While it 's genuinely cool to hear characters talk about early rap records -LRB- Sugar Hill <unk> , etc. -RRB- , the constant <unk> of hip-hop <unk> can alienate even the <unk> audiences . <pad>\n",
      "1 0.421931 Not the kind of film that will appeal to a mainstream American audience , but there is a certain charm about the film that makes it a suitable entry into the fest circuit . <pad>\n",
      "1 0.0310935 Whether you like rap music or loathe it , you ca n't deny either the tragic loss of two young men in the prime of their talent or the power of this movie . <pad>\n",
      "0 0.954168 Pumpkin means to be an outrageous dark satire on <unk> life , but its ambitions far <unk> the abilities of writer Adam Larson Broder and his <unk> , Tony R. <unk> , in their feature debut .\n",
      "0 0.998075 The tale of <unk> -LRB- <unk> Lau -RRB- , a sleek <unk> on the trail of O -LRB- Takashi <unk> -RRB- , the most legendary of Asian <unk> , is too <unk> to take hold . <pad>\n",
      "1 1.67298e-07 It 's a demented kitsch mess -LRB- although the smeary digital video does match the muddled narrative -RRB- , but it 's savvy about celebrity and has more guts and energy than much of what will open this year . <pad>\n",
      "0 0.999418 It 's inoffensive , <unk> , built to inspire the young people , set to an <unk> soundtrack of beach party pop numbers and aside from its remarkable camerawork and awesome scenery , it 's about as exciting as a <unk> . <pad> <pad> <pad>\n",
      "1 0.219354 Whether writer-director Anne Fontaine 's film is a ghost story , an account of a nervous breakdown , a trip down memory lane , all three or none of the above , it is as seductive as it is haunting . <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for label, p, text in zip(true, pred, errors):\n",
    "    print(label, p, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
