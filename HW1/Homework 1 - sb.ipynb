{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be building several varieties of text classifiers.\n",
    "\n",
    "## Goal\n",
    "\n",
    "We ask that you construct the following models in PyTorch:\n",
    "\n",
    "1. A naive Bayes unigram classifer (follow Wang and Manning http://www.aclweb.org/anthology/P/P12/P12-2.pdf#page=118: you should only implement Naive Bayes, not the combined classifer with SVM).\n",
    "2. A logistic regression model over word types (you can implement this as $y = \\sigma(\\sum_i W x_i + b)$) \n",
    "3. A continuous bag-of-word neural network with embeddings (similar to CBOW in Mikolov et al https://arxiv.org/pdf/1301.3781.pdf).\n",
    "4. A simple convolutional neural network (any variant of CNN as described in Kim http://aclweb.org/anthology/D/D14/D14-1181.pdf).\n",
    "5. Your own extensions to these models...\n",
    "\n",
    "Consult the papers provided for hyperparameters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. You may construct your models inline or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in c:\\programdata\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable(array, requires_grad=False):\n",
    "    if isinstance(array, np.ndarray):\n",
    "        return t.autograd.Variable(t.from_numpy(array), requires_grad=requires_grad)\n",
    "    elif isinstance(array, list) or isinstance(array,tuple):\n",
    "        return t.autograd.Variable(t.from_numpy(np.array(array)), requires_grad=requires_grad)\n",
    "    elif isinstance(array, float) or isinstance(array, int):\n",
    "        return t.autograd.Variable(t.from_numpy(np.array([array])), requires_grad=requires_grad)\n",
    "    elif isinstance(array, t.Tensor):\n",
    "        return t.autograd.Variable(array, requires_grad=requires_grad)\n",
    "    else: raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use of this problem is known as the Stanford Sentiment Treebank (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf). It is a variant of a standard sentiment classification task. For simplicity, we will use the most basic form. Classifying a sentence as positive or negative in sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, `torchtext` requires that we define a mapping from the raw text data to featurized indices. These fields make it easy to map back and forth between readable data and math, which helps for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we input our data. Here we will use the standard SST train split, and tell it the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this data. It's still in its original form, we can see that each example consists of a label and the original words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 6920\n",
      "vars(train[0]) {'text': ['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'], 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "print('len(train)', len(train_dataset))\n",
    "print('vars(train[0])', vars(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to map this data to features, we need to assign an index to each word an label. The function build vocab allows us to do this and provides useful options that we will need in future assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 16286\n",
      "len(LABEL.vocab) 3\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train_dataset)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to create batches of our training data that can be used for training and validating the model. This function produces 3 iterators that will let us go through the train, val and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train_dataset, val_dataset, test_dataset), batch_size=10, device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a single batch from one of these iterators. The library automatically converts the underlying words into indices. It then produces tensors for batches of x and y. In this case it will consist of the number of words of the longest sentence (with padding) followed by the number of batches. We can use the vocabulary dictionary to convert back from these indices to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch [max sent length, batch size] torch.Size([30, 10])\n",
      "Second in batch Variable containing:\n",
      "   363\n",
      "     4\n",
      "  1247\n",
      " 10242\n",
      "    32\n",
      "  8128\n",
      "  5369\n",
      "  3895\n",
      "    10\n",
      "   994\n",
      "    67\n",
      "   415\n",
      "    27\n",
      " 13588\n",
      "    20\n",
      "    23\n",
      "   896\n",
      "   636\n",
      "   206\n",
      "    32\n",
      "    96\n",
      "    10\n",
      "   114\n",
      "   611\n",
      "     5\n",
      "   239\n",
      "   757\n",
      "    35\n",
      "   422\n",
      "     2\n",
      "[torch.LongTensor of size 30]\n",
      "\n",
      "Converted back to string:  Despite the surface attractions -- Conrad L. Hall 's cinematography will likely be nominated for an Oscar next year -- there 's something impressive and yet lacking about everything .\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Size of text batch [max sent length, batch size]\", batch.text.size())\n",
    "print(\"Second in batch\", batch.text[:, 0])\n",
    "print(\"Converted back to string: \", \" \".join([TEXT.vocab.itos[i] for i in batch.text[:, 0].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly it produces a vector for each of the labels in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of label batch [batch size] torch.Size([10])\n",
      "Second in batch Variable containing:\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      "\n",
      "Converted back to string:  positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of label batch [batch size]\", batch.label.size())\n",
    "print(\"Second in batch\", batch.label[0])\n",
    "print(\"Converted back to string: \", LABEL.vocab.itos[batch.label.data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the Vocab object can be used to map pretrained word vectors to the indices in the vocabulary. This will be very useful for part 3 and 4 of the problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings size  torch.Size([16286, 300])\n",
      "Word embedding of 'follows', first 10 dim  \n",
      " 0.3925\n",
      "-0.4770\n",
      " 0.1754\n",
      "-0.0845\n",
      " 0.1396\n",
      " 0.3722\n",
      "-0.0878\n",
      "-0.2398\n",
      " 0.0367\n",
      " 0.2800\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))\n",
    "\n",
    "print(\"Word embeddings size \", TEXT.vocab.vectors.size())\n",
    "print(\"Word embedding of 'follows', first 10 dim \", TEXT.vocab.vectors[TEXT.vocab.stoi['follows']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "Using the data given by this iterator, you should construct 4 different torch models that take in batch.text and produce a distribution over labels. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition:  https://www.kaggle.com/c/harvard-cs281-hw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \"All models should be able to be run with following command.\"\n",
    "    upload = []\n",
    "    # Update: for kaggle the bucket iterator needs to have batch_size 10\n",
    "#     test_iter = torchtext.data.BucketIterator(test_dataset, train=False, batch_size=10)\n",
    "    for batch in test_iter:\n",
    "        # Your prediction data here (don't cheat!)\n",
    "        probs = NB(batch.text).long()\n",
    "        upload += list(probs.data)\n",
    "\n",
    "    with open(\"predictions.txt\", \"w\") as f:\n",
    "        for u in upload:\n",
    "            f.write(str(u) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you should put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model: NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_sentence(batch, vec_dim=300, sentence_length=16):\n",
    "    \"\"\"Convert integer-encoded sentence to word vector representation\"\"\"\n",
    "    return t.cat([TEXT.vocab.vectors[batch.text.data.long()[:,i]].view(1,sentence_length,vec_dim) for i in range(batch.batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the NB weights. Then you classify with t.sign(t.sum(W(text)) + bias)\n",
    "# You can do that because the features are the indicators variables of the word occurences\n",
    "W = t.nn.Embedding(len(TEXT.vocab), 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter.batch_size = 10\n",
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████████████████████████████████████████▊    | 655/692 [00:01<00:00, 543.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "i = 0\n",
    "pos = 0\n",
    "neg = 0\n",
    "\n",
    "# count the occurences of each word (classwise)\n",
    "for b in tqdm(train_iter):\n",
    "    i += 1\n",
    "    pos_tmp = t.nonzero((b.label==1).data.long()).numpy().flatten().shape[0]\n",
    "    neg_tmp = t.nonzero((b.label==2).data.long()).numpy().flatten().shape[0]\n",
    "    pos += pos_tmp\n",
    "    neg += neg_tmp\n",
    "    if neg_tmp < 10:\n",
    "        positive_counts += Counter(b.text.transpose(0,1).index_select(0, t.nonzero((b.label==1).data.long()).squeeze()).data.numpy().flatten().tolist())\n",
    "    if pos_tmp < 10:\n",
    "        negative_counts += Counter(b.text.transpose(0,1).index_select(0, t.nonzero((b.label==2).data.long()).squeeze()).data.numpy().flatten().tolist())\n",
    "    if i >= 692:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(len(TEXT.vocab)):  # pseudo counts\n",
    "    positive_counts[k] += 1\n",
    "    negative_counts[k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale_pos = sum(list(positive_counts.values()))\n",
    "scale_neg = sum(list(negative_counts.values()))\n",
    "positive_prop = {k: v/scale_pos for k,v in positive_counts.items()}\n",
    "negative_prop = {k: v/scale_neg for k,v in negative_counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = {k: np.log(positive_prop[k] / negative_prop[k]) for k in range(len(TEXT.vocab))}\n",
    "W.weight.data = t.from_numpy(np.array([r[k] for k in range(len(TEXT.vocab))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bias = np.log(pos/neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NB(text):\n",
    "    \"\"\"sign(Wx + b)\"\"\"\n",
    "    return t.sign(t.cat([t.sum(W(text.transpose(0,1)[i])) for i in range(text.data.numpy().shape[1])]) + bias).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "upload = []\n",
    "true = []\n",
    "for batch in test_iter:\n",
    "    # Your prediction data here (don't cheat!)\n",
    "    probs = NB(batch.text).long()\n",
    "    upload += list(probs.data)\n",
    "    true += batch.label.data.numpy().tolist()\n",
    "true = [x if x == 1 else -1 for x in true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8237232289950577"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([(x*y == 1) for x,y in zip(upload,true)])/ len(upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = t.nn.Embedding(len(TEXT.vocab), 1)\n",
    "b = variable(0., True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "nll = t.nn.NLLLoss(size_average=True)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimizer = t.optim.RMSprop([b, W.weight], lr=learning_rate)\n",
    "sig = t.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(692, 10)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iter), train_iter.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def eval_perf(iterator):\n",
    "    count = 0\n",
    "    bs = iterator.batch_size * 1\n",
    "    iterator.batch_size = 1\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # get data\n",
    "        y_pred = (sig(t.cat([W(batch.text.transpose(0,1)[i]).sum() for i in range(batch.text.data.numpy().shape[1])]) + b.float()) > 0.5).long()\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "\n",
    "        count += t.sum((y == y_pred).long())\n",
    "        if i >= len(iterator) - 1:\n",
    "            break\n",
    "    iterator.batch_size = bs\n",
    "    return (count.float() / len(iterator)).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy after 0 epochs: 0.65\n",
      "Validation accuracy after 1 epochs: 0.70\n",
      "Validation accuracy after 2 epochs: 0.73\n",
      "Validation accuracy after 3 epochs: 0.73\n",
      "Validation accuracy after 4 epochs: 0.74\n",
      "Validation accuracy after 5 epochs: 0.74\n",
      "Validation accuracy after 6 epochs: 0.75\n",
      "Validation accuracy after 7 epochs: 0.76\n",
      "Validation accuracy after 8 epochs: 0.77\n",
      "Validation accuracy after 9 epochs: 0.77\n",
      "Validation accuracy after 10 epochs: 0.78\n",
      "Validation accuracy after 11 epochs: 0.78\n",
      "Validation accuracy after 12 epochs: 0.78\n",
      "Validation accuracy after 13 epochs: 0.78\n",
      "Validation accuracy after 14 epochs: 0.78\n",
      "Validation accuracy after 15 epochs: 0.78\n",
      "Validation accuracy after 16 epochs: 0.78\n",
      "Validation accuracy after 17 epochs: 0.77\n",
      "Validation accuracy after 18 epochs: 0.77\n",
      "Validation accuracy after 19 epochs: 0.78\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "n_epochs = 20\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # get data\n",
    "        y_pred = sig(t.cat([W(batch.text.transpose(0,1)[i]).sum() for i in range(batch.text.data.numpy().shape[1])]) + b.float()).unsqueeze(1)\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "        \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss\n",
    "        y_pred = t.cat([1-y_pred, y_pred], 1).float()  # nll needs two inputs: the prediction for the negative/positive classes\n",
    "\n",
    "        loss = nll.forward(y_pred, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "        if i >= len(train_iter) - 1:\n",
    "            break\n",
    "    train_iter.init_epoch()\n",
    "    print(\"Validation accuracy after %d epochs: %.2f\" % (_, eval_perf(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(text, vdim=300):\n",
    "    length, batch_size = text.data.numpy().shape\n",
    "    return t.mean(t.cat([TEXT.vocab.vectors[text.long().data.transpose(0,1)[i]].view(1,length,vdim) for i in range(batch_size)]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = variable(np.random.normal(0, .1, (300,)), True)\n",
    "b = variable(0., True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "nll = t.nn.NLLLoss(size_average=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = t.optim.RMSprop([b, W], lr=learning_rate)\n",
    "sig = t.nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def eval_perf(iterator):\n",
    "    count = 0\n",
    "    bs = iterator.batch_size * 1\n",
    "    iterator.batch_size = 1\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # get data\n",
    "        text_ = batch.text\n",
    "        length = text_.data.numpy().shape[0]\n",
    "        y_pred = (sig(t.mm(variable(vectorize(text_)),W.float().resize(300,1)).squeeze() + b.float().squeeze()) > 0.5).long()\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "\n",
    "        count += t.sum((y == y_pred).long())\n",
    "        if i >= len(iterator) - 1:\n",
    "            break\n",
    "    iterator.batch_size = bs\n",
    "    return (count.float() / len(iterator)).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy after 0 epochs: 0.71\n",
      "Validation accuracy after 1 epochs: 0.70\n",
      "Validation accuracy after 2 epochs: 0.70\n",
      "Validation accuracy after 3 epochs: 0.71\n",
      "Validation accuracy after 4 epochs: 0.72\n",
      "Validation accuracy after 5 epochs: 0.71\n",
      "Validation accuracy after 6 epochs: 0.69\n",
      "Validation accuracy after 7 epochs: 0.71\n",
      "Validation accuracy after 8 epochs: 0.71\n",
      "Validation accuracy after 9 epochs: 0.71\n",
      "Validation accuracy after 10 epochs: 0.71\n",
      "Validation accuracy after 11 epochs: 0.71\n",
      "Validation accuracy after 12 epochs: 0.71\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-254-2ab122ca28b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 98\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "n_epochs = 20\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # get data\n",
    "        text_ = batch.text\n",
    "        length = text_.data.numpy().shape[0]\n",
    "        y_pred = sig(t.mm(variable(vectorize(text_)),W.float().resize(300,1)).squeeze() + b.float().squeeze()).unsqueeze(1)\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "        \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss\n",
    "        y_pred = t.cat([1-y_pred, y_pred], 1).float()  # nll needs two inputs: the prediction for the negative/positive classes\n",
    "\n",
    "        loss = nll.forward(y_pred, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "        if i >= len(train_iter) - 1:\n",
    "            break\n",
    "    train_iter.init_epoch()\n",
    "    print(\"Validation accuracy after %d epochs: %.2f\" % (_, eval_perf(val_iter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model 4: convnet on word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import Conv1d as conv, MaxPool1d as maxpool, Linear as fc, Softmax, ReLU, Dropout, Tanh, BatchNorm1d as BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = conv(300, 100, 2)\n",
    "layer2 = fc(100, 100)\n",
    "layer3 = fc(100, 2)\n",
    "softmax = Softmax()\n",
    "dropout = Dropout()\n",
    "relu = ReLU()\n",
    "tanh = Tanh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Convnet(t.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Convnet, self).__init__()\n",
    "        self.conv1 = conv(300, 100, 5, padding=2)\n",
    "#         self.conv2 = conv(100, 100, 3, padding=1)\n",
    "        self.fc1 = fc(100, 100)\n",
    "        self.fc2 = fc(100, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        dropout = Dropout(.2)\n",
    "        xx = self.conv1(x)\n",
    "#         xx = tanh(xx)\n",
    "#         xx = self.conv2(xx)\n",
    "        xx = tanh(t.max(xx, -1)[0])\n",
    "        xx = self.fc1(xx)\n",
    "#         xx = dropout(xx)\n",
    "        xx = tanh(xx)\n",
    "        xx = self.fc2(xx)\n",
    "        xx = dropout(xx)\n",
    "        return softmax(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet(x):\n",
    "    xx = layer1(x)\n",
    "    xx = tanh(t.max(xx, -1)[0])\n",
    "    xx = layer2(xx)\n",
    "    xx = tanh(xx)\n",
    "    xx = layer3(xx)\n",
    "    return softmax(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "nll = t.nn.NLLLoss(size_average=True)\n",
    "learning_rate = 1e-4\n",
    "convnet = Convnet()\n",
    "optimizer = t.optim.Adam(convnet.parameters(), lr=learning_rate, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(text, vdim=300):\n",
    "    length, batch_size = text.data.numpy().shape\n",
    "    return t.cat([TEXT.vocab.vectors[text.long().data.transpose(0,1)[i]].view(1,length,vdim) for i in range(batch_size)]).permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "\n",
    "def eval_perf(iterator):\n",
    "    count = 0\n",
    "    bs = iterator.batch_size * 1\n",
    "    iterator.batch_size = 1\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # get data\n",
    "        text = batch.text\n",
    "        y_pred = (convnet(variable(vectorize(text)))[:, 1] > 0.5).long()\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "\n",
    "        count += t.sum((y == y_pred).long())\n",
    "        if i >= len(iterator) - 1:\n",
    "            break\n",
    "    iterator.batch_size = bs\n",
    "    return (count.float() / len(iterator)).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy after 0 epochs: 0.60\n",
      "Validation accuracy after 1 epochs: 0.72\n",
      "Validation accuracy after 2 epochs: 0.74\n",
      "Validation accuracy after 3 epochs: 0.74\n",
      "Validation accuracy after 4 epochs: 0.75\n",
      "Validation accuracy after 5 epochs: 0.76\n",
      "Validation accuracy after 6 epochs: 0.73\n",
      "Validation accuracy after 7 epochs: 0.76\n",
      "Validation accuracy after 8 epochs: 0.75\n",
      "Validation accuracy after 9 epochs: 0.75\n",
      "Validation accuracy after 10 epochs: 0.75\n",
      "Validation accuracy after 11 epochs: 0.75\n",
      "Validation accuracy after 12 epochs: 0.75\n",
      "Validation accuracy after 13 epochs: 0.75\n",
      "Validation accuracy after 14 epochs: 0.75\n",
      "Validation accuracy after 15 epochs: 0.75\n",
      "Validation accuracy after 16 epochs: 0.75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-277-a97dd8d33f26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m--> 156\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 98\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "n_epochs = 25\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        # get data\n",
    "        text = batch.text\n",
    "        y_pred = convnet(variable(vectorize(text)))\n",
    "        y = batch.label.long()*(-1) + 2\n",
    "        \n",
    "        # initialize gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # loss\n",
    "        loss = nll.forward(y_pred, y)\n",
    "\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "                \n",
    "        if i >= len(train_iter) - 1:\n",
    "            break\n",
    "    train_iter.init_epoch()\n",
    "    model.eval()\n",
    "    print(\"Validation accuracy after %d epochs: %.2f\" % (_, eval_perf(val_iter)))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.0000e+00  2.3268e-21\n",
       " 1.0000e+00  4.0393e-23\n",
       " 2.1157e-20  1.0000e+00\n",
       " 1.0000e+00  4.6889e-25\n",
       " 1.0000e+00  1.6023e-11\n",
       " 1.0000e+00  6.1515e-29\n",
       " 2.4236e-25  1.0000e+00\n",
       " 1.0000e+00  7.2904e-25\n",
       " 7.1563e-14  1.0000e+00\n",
       " 3.4280e-26  1.0000e+00\n",
       "[torch.FloatTensor of size 10x2]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convnet(variable(vectorize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2\n",
       " 2\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 1\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
