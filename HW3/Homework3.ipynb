{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': <torchtext.data.field.Field object at 0x7f128ee04160>, 'src': <torchtext.data.field.Field object at 0x7f128ee041d0>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "    99     26   3550     40    241     23     20     28      9     28     12\n",
      "  2482    188     24     13    497     27    105    186    751    442    106\n",
      "     4     71     20    448      4      0    164     92    107     64     15\n",
      "     7    440    105    174     29   1251    161      0   1259     65     70\n",
      "  5037      3      7     18      0    389   1829      0     68     22    600\n",
      "  1050     18      0   9699   2750  10211    158     33      0    335   1741\n",
      "     2     16      2      2      2      2      2      2      2      2      2\n",
      "\n",
      "Columns 11 to 21 \n",
      "   569     39   1064     73     39    373   1738     12     39    104     26\n",
      "    13   7219    364      3   1142    672     13     29     96      4   6815\n",
      "    22      4   2083   2211      7   9185    106    599      4      5     83\n",
      "  1978    269    140     35   1805    659     15     55     98   4381     72\n",
      "   576    113   1102     10     35      0     70   3088     41    149      0\n",
      "   831    961     16      0   3078     27      0   1306  11195      0    264\n",
      "    16      2     17      2      2      2      2      2      2      2      2\n",
      "\n",
      "Columns 22 to 31 \n",
      "    23   1212     23     40      9      0     12    280    102    102\n",
      "     4    143     22      5    613    187      5     10   5026    678\n",
      "    91     42    262   1175     32     53      0      0     78     10\n",
      "    18    236     53      4    223     56    195     81     53      6\n",
      "    64    450   4085    982      0    981      6   6034      7      0\n",
      "  7911      0      0    696    789    446      0     63   1093  12185\n",
      "     2      2      2      2      2      2      2      2      2      2\n",
      "[torch.LongTensor of size 7x32]\n",
      "\n",
      "Target\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "   42    14  5550    41   803    48    10    34    57    34    14   436    34\n",
      "  132    16    39    19   381    11    75    26   238   146    91    19    28\n",
      "   11    12    10   111    11     8    54    30     7   184    10   340     8\n",
      "    6  3772    75    30  1874     0    73    90     6    35    90     8  8273\n",
      " 1243     5     6  6491    38  1265     5     7   493    47     7   751     9\n",
      " 1292    11     0    16     0    29    73    28  7173   387   184   579    35\n",
      "    4    30   338     4     4     0  4931     8    12     9    35    21   114\n",
      "    3    16     4     3     3     4    38  8041   829    16   122     3  1690\n",
      "    1    21     3     1     1     3    68  1035     4     4  1759     1     4\n",
      "    1     3     1     1     1     1     4    33     3     3   143     1     3\n",
      "    1     1     1     1     1     1     3     6     1     1     4     1     1\n",
      "    1     1     1     1     1     1     1   619     1     1     3     1     1\n",
      "    1     1     1     1     1     1     1     4     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     3     1     1     1     1     1\n",
      "\n",
      "Columns 13 to 25 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "   89    24     0    52    76    14    42   200  3714   128    14    48    41\n",
      "  148     5    79    72     5    38   109    12    39   223   127    11     6\n",
      "    9  8853  5159     5     8    92    11     6    27    20    15    92   798\n",
      "  404     7  3038   130   131    53    54  3006  1618    11    90   140    11\n",
      "   25   144     4   405    78  1068    61    15     8    30   122    60  6755\n",
      "   15     5     3     6   560  1293     0    86  2833    54    13  3611     4\n",
      "   65    16     1  9869   123     4     4    60   203  2665     0     0     3\n",
      "    7    23     1    25     4     3     3     6     4     4    82     4     1\n",
      "  547   888     1   213     3     1     1  3333     3     3   534     3     1\n",
      "   21     4     1     0     1     1     1  2051     1     1     4     1     1\n",
      "   22     3     1     4     1     1     1     4     1     1     3     1     1\n",
      "    3     1     1     3     1     1     1     3     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 26 to 31 \n",
      "    2     2     2     2     2     2\n",
      "   52     0    14   305    52    57\n",
      "  267     0     6    16    86   230\n",
      "  148    11  7357   325    55    16\n",
      "    9    60  3028  6820    71     6\n",
      "    0  1406   441    99     9     0\n",
      " 3513   433     6    11  1048  4529\n",
      "    4     4   184    16     4     4\n",
      "    3     3     9   325     3     3\n",
      "    1     1     6     0     1     1\n",
      "    1     1  1388    21     1     1\n",
      "    1     1     4     3     1     1\n",
      "    1     1     3     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "[torch.LongTensor of size 15x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Als ich in meinen 20ern war , hatte ich meine erste Psychotherapie-Patientin .\r\n",
      "Ich war Doktorandin und studierte Klinische Psychologie in Berkeley .\r\n",
      "Sie war eine 26-jährige Frau namens Alex .\r\n",
      "Und als ich das hörte , war ich erleichtert .\r\n",
      "Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten .\r\n",
      "Und ich bekam eine Frau in den 20ern , die über Jungs reden wollte .\r\n",
      "Das kriege ich hin , dachte ich mir .\r\n",
      "Aber ich habe es nicht hingekriegt .\r\n",
      "Arbeit kam später , Heiraten kam später , Kinder kamen später , selbst der Tod kam später .\r\n",
      "Leute in den 20ern wie Alex und ich hatten nichts als Zeit .\r\n"
     ]
    }
   ],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../HW3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "os.chdir('../HW3')  # so that there is not any import bug in case HW2 is not already the working directory\n",
    "from utils import *\n",
    "from const import *\n",
    "import argparse\n",
    "import torch as t\n",
    "from process_params import check_args, get_params\n",
    "from const import *\n",
    "from data_process import generate_iterators, generate_text\n",
    "from utils import *\n",
    "t.manual_seed(1)\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from utils import variable\n",
    "from const import *\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import spacy\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import pickle\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMA(t.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of `Neural Machine Translation by Jointly Learning to Align and Translate`\n",
    "    https://arxiv.org/abs/1409.0473\n",
    "\n",
    "    NOTE THAT ITS INPUT SHOULD HAVE THE BATCH SIZE FIRST !!!!!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, source_embeddings=None, target_embeddings=None):\n",
    "        super(LSTMA, self).__init__()\n",
    "        print(\"Initializing LSTMA\")\n",
    "        self.cuda_flag = params.get('cuda', CUDA_DEFAULT)\n",
    "        self.model_str = 'LSTMA'\n",
    "        self.params = params\n",
    "\n",
    "        # Initialize hyperparams.\n",
    "        self.hidden_dim = params.get('hidden_dim', 100)\n",
    "        self.batch_size = params.get('batch_size', 32)\n",
    "        try:\n",
    "            # if you provide pre-trained embeddings for target/source, they should have the same embedding dim\n",
    "            assert source_embeddings.size(1) == target_embeddings.size(1)\n",
    "            self.embedding_dim = source_embeddings.size(1)\n",
    "            self.source_vocab_size = params.get('source_vocab_size')\n",
    "            self.target_vocab_size = params.get('target_vocab_size')\n",
    "        except:\n",
    "            # if you dont provide a pre-trained embedding, you have to provide these\n",
    "            self.embedding_dim = params.get('embedding_dim')\n",
    "            self.source_vocab_size = params.get('source_vocab_size')\n",
    "            self.target_vocab_size = params.get('target_vocab_size')\n",
    "            assert self.embedding_dim is not None and self.source_vocab_size is not None and self.target_vocab_size is not None\n",
    "        self.output_size = self.target_vocab_size\n",
    "        self.num_layers = params.get('num_layers', 1)\n",
    "        self.dropout = params.get('dropout', 0.5)\n",
    "        self.embed_dropout = params.get('embed_dropout')\n",
    "        self.train_embedding = params.get('train_embedding', True)\n",
    "\n",
    "        # Initialize embeddings. Static embeddings for now.\n",
    "        self.source_embeddings = t.nn.Embedding(self.source_vocab_size, self.embedding_dim)\n",
    "        self.target_embeddings = t.nn.Embedding(self.target_vocab_size, self.embedding_dim)\n",
    "        if source_embeddings is not None:\n",
    "            self.source_embeddings.weight = t.nn.Parameter(source_embeddings, requires_grad=self.train_embedding)\n",
    "        if target_embeddings is not None:\n",
    "            self.target_embeddings.weight = t.nn.Parameter(target_embeddings, requires_grad=self.train_embedding)\n",
    "\n",
    "        # Initialize network modules.\n",
    "        # note that the encoder is a BiLSTM. The output is modified by the fact that the hidden dim is doubled, and if you set\n",
    "        # the number of layers to L, there will actually be 2L layers (the forward ones and the backward ones). Consequently the first\n",
    "        # dimension of the hidden outputs of the forward pass (the 2nd output in the tuple) will be a tuple of\n",
    "        # 2 tensors having as first dim twice the hidden dim you set\n",
    "        self.encoder_rnn = t.nn.LSTM(self.embedding_dim, self.hidden_dim // 2, dropout=self.dropout, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        self.decoder_rnn = t.nn.LSTM(self.embedding_dim, self.hidden_dim, dropout=self.dropout, num_layers=self.num_layers, batch_first=True)\n",
    "        self.hidden_dec_initializer = t.nn.Linear(self.hidden_dim // 2, self.hidden_dim)\n",
    "        self.hidden2out = t.nn.Linear(self.hidden_dim * 2, self.output_size)\n",
    "        if self.embed_dropout:\n",
    "            self.dropout_1s = t.nn.Dropout(self.dropout)\n",
    "            self.dropout_1t = t.nn.Dropout(self.dropout)\n",
    "        self.dropout_2 = t.nn.Dropout(self.dropout)\n",
    "        self.lsm = nn.LogSoftmax()\n",
    "        \n",
    "        self.beam_size = params.get('beam_size',3)\n",
    "        self.max_beam_depth = params.get('max_beam_depth',20)\n",
    "\n",
    "        if self.cuda_flag:\n",
    "            self = self.cuda()\n",
    "\n",
    "    # @todo: maybe this is wrong in case of deep LSTM DECODER (I am not sure the dimensions are correct)\n",
    "    def init_hidden(self, data, type, batch_size=None):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state, either for the encoder or the decoder\n",
    "\n",
    "        For type=`enc`, it should just be initialized with 0s\n",
    "        For type=`dec`, it should be initialized with tanh(W h1_backward) (see page 13 of the paper, last paragraph)\n",
    "\n",
    "        `data` is either something you initialize the hidden state with, or None\n",
    "        \"\"\"\n",
    "        bs = batch_size if batch_size is not None else self.batch_size\n",
    "        if type == 'dec':\n",
    "            # in that case, `data` is the output of the encoder\n",
    "            # data[:, :1, self.hidden_dim // 2:]\n",
    "            # `:` for the whole batch\n",
    "            # `:1` because you want the hidden state of the first time step (see paper, they use backward(h1))\n",
    "            # but also `self.hidden_dim // 2:`, because you want the backward part only (the last coefficients)\n",
    "            h = F.tanh(self.hidden_dec_initializer(data[:, :1, self.hidden_dim // 2:]))  # @todo: verify that the last hdim/2 weights actually correspond to the backward layer(s)\n",
    "            h = h.transpose(1, 0)\n",
    "            return (\n",
    "                h,\n",
    "                variable(np.zeros((self.num_layers, bs, self.hidden_dim)), cuda=self.cuda_flag)\n",
    "            )\n",
    "        elif type == 'enc':\n",
    "            # in that case data is None\n",
    "            return tuple((\n",
    "                variable(np.zeros((self.num_layers * 2, bs, self.hidden_dim // 2)), cuda=self.cuda_flag),\n",
    "                variable(np.zeros((self.num_layers * 2, bs, self.hidden_dim // 2)), cuda=self.cuda_flag)\n",
    "            ))\n",
    "        else:\n",
    "            raise ValueError('the type should be either `dec` or `enc`')\n",
    "\n",
    "    def forward(self, x_source, x_target):\n",
    "        # EMBEDDING\n",
    "        embedded_x_source = self.source_embeddings(x_source)\n",
    "        embedded_x_target = self.target_embeddings(x_target[:, :-1])  # don't make a prediction for the word following the last one\n",
    "        if self.embed_dropout:\n",
    "            embedded_x_source = self.dropout_1s(embedded_x_source)\n",
    "            embedded_x_target = self.dropout_1t(embedded_x_target)\n",
    "\n",
    "        # RECURRENT\n",
    "        hidden = self.init_hidden(None, 'enc', x_source.size(0))\n",
    "        enc_out, _ = self.encoder_rnn(embedded_x_source, hidden)\n",
    "        hidden = self.init_hidden(enc_out, 'dec', x_source.size(0))\n",
    "        dec_out, _ = self.decoder_rnn(embedded_x_target, hidden)\n",
    "\n",
    "        # ATTENTION\n",
    "        scores = t.bmm(enc_out, dec_out.transpose(1, 2))  # this will be a batch x source_len x target_len\n",
    "        attn_dist = F.softmax(scores, dim=1)  # batch x source_len x target_len\n",
    "        context = t.bmm(attn_dist.permute(0, 2, 1), enc_out)  # batch x target_len x hidden_dim\n",
    "\n",
    "        # OUTPUT\n",
    "        # concatenate the output of the decoder and the context and apply nonlinearity\n",
    "        pred = F.tanh(t.cat([dec_out, context], -1))  # @todo : tanh necessary ?\n",
    "        pred = self.dropout_2(pred)  # batch x target_len x 2 hdim\n",
    "        pred = self.hidden2out(pred)\n",
    "        return pred\n",
    "\n",
    "    def translate(self, x_source):\n",
    "        self.eval()\n",
    "\n",
    "        # EMBEDDING\n",
    "        embedded_x_source = self.source_embeddings(x_source)\n",
    "        if self.embed_dropout:\n",
    "            embedded_x_source = self.dropout_1s(embedded_x_source)\n",
    "\n",
    "        # RECURRENT\n",
    "        hidden = self.init_hidden(None, 'enc')\n",
    "        enc_out, _ = self.encoder_rnn(embedded_x_source, hidden)\n",
    "        hidden = self.init_hidden(enc_out, 'dec')\n",
    "        x_target = (SOS_TOKEN * t.ones(x_source.size(0), 1)).long()  # `2` is the SOS token (<s>)\n",
    "        x_target = variable(x_target, to_float=False, cuda=self.cuda_flag)\n",
    "        count_eos = 0\n",
    "        time = 0\n",
    "        while count_eos < x_source.size(0):\n",
    "            embedded_x_target = self.target_embeddings(x_target)\n",
    "            dec_out, hidden = self.decoder_rnn(embedded_x_target, hidden)\n",
    "            hidden = hidden[0].detach(), hidden[1].detach()\n",
    "            dec_out = dec_out[:, time:time + 1, :].detach()\n",
    "\n",
    "            # ATTENTION\n",
    "            scores = t.bmm(enc_out, dec_out.transpose(1, 2))  # this will be a batch x source_len x target_len\n",
    "            try:\n",
    "                attn_dist = F.softmax(scores, dim=1)  # batch x source_len x target_len\n",
    "            except:\n",
    "                attn_dist = F.softmax(scores.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "            context = t.bmm(attn_dist.permute(0, 2, 1), enc_out)  # batch x target_len x hidden_dim\n",
    "\n",
    "            # OUTPUT\n",
    "            # concatenate the output of the decoder and the context and apply nonlinearity\n",
    "            pred = F.tanh(t.cat([dec_out, context], -1))  # @todo : tanh necessary ?\n",
    "            pred = self.dropout_2(pred)  # batch x target_len x 2 hdim\n",
    "            pred = self.hidden2out(pred).detach()\n",
    "            x_target = t.cat([x_target, pred.max(2)[1]], 1).detach()\n",
    "\n",
    "            # should you stop ?\n",
    "            count_eos += t.sum((pred.max(2)[1] == EOS_TOKEN).long()).data.cpu().numpy()[0]  # `3` is the EOS token\n",
    "            time += 1\n",
    "        return x_target\n",
    "    \n",
    "    def translate_beam(self,x_source):\n",
    "        self.eval()        \n",
    "\n",
    "        # EMBEDDING\n",
    "        embedded_x_source = self.source_embeddings(x_source)\n",
    "        if self.embed_dropout:\n",
    "            embedded_x_source = self.dropout_1s(embedded_x_source)\n",
    "        \n",
    "        terminate_beam = False\n",
    "        batch_size = x_source.size(0)\n",
    "        \n",
    "        # RECURRENT\n",
    "        hidden = self.init_hidden(None, 'enc')\n",
    "        enc_out, _ = self.encoder_rnn(embedded_x_source, hidden)\n",
    "        hidden = self.init_hidden(enc_out, 'dec')\n",
    "        x_target = SOS_TOKEN * np.ones((x_source.size(0), 1))  # `2` is the SOS token (<s>)\n",
    "        count_eos = 0\n",
    "        time = 0\n",
    "        \n",
    "        \n",
    "        #INIT SOME STUFF.\n",
    "        self.beam = np.array([x_target])\n",
    "        self.beam_scores = np.zeros((batch_size,1))\n",
    "        \n",
    "        while not terminate_beam and time < self.max_beam_depth: \n",
    "            \n",
    "            collective_children   = np.array([])\n",
    "            collective_scores     = np.array([])\n",
    "           \n",
    "            if len(self.beam) == 1:\n",
    "                reshaped_beam = self.beam\n",
    "            else:\n",
    "                reshaped_beam = self.beam.reshape((self.beam_size,batch_size,time+1))\n",
    "                \n",
    "            for it, elem in enumerate(reshaped_beam) : \n",
    "                elem = t.from_numpy(elem).long()\n",
    "                x_target = elem.view(self.batch_size,-1)\n",
    "                x_target = variable(x_target, to_float=False, cuda=self.cuda_flag).long()\n",
    "                embedded_x_target = self.target_embeddings(x_target)\n",
    "                dec_out, hidden = self.decoder_rnn(embedded_x_target, hidden)\n",
    "                hidden = hidden[0].detach(), hidden[1].detach()\n",
    "                dec_out = dec_out[:, time:time + 1, :].detach()\n",
    "    \n",
    "                # ATTENTION\n",
    "                scores = t.bmm(enc_out, dec_out.transpose(1, 2))  # this will be a batch x source_len x target_len\n",
    "                try:\n",
    "                    attn_dist = F.softmax(scores, dim=1)  # batch x source_len x target_len\n",
    "                except:\n",
    "                    attn_dist = F.softmax(scores.permute(1, 0, 2)).permute(1, 0, 2)\n",
    "                context = t.bmm(attn_dist.permute(0, 2, 1), enc_out)  # batch x target_len x hidden_dim\n",
    "    \n",
    "                # OUTPUT\n",
    "                # concatenate the output of the decoder and the context and apply nonlinearity\n",
    "                pred = F.tanh(t.cat([dec_out, context], -1))  # @todo : tanh necessary ?\n",
    "                pred = self.dropout_2(pred)  # batch x target_len x 2 hdim\n",
    "                pred = self.hidden2out(pred).detach()\n",
    "                pred = self.lsm(pred).detach()\n",
    "\n",
    "                topk = t.topk(pred, self.beam_size,dim=2)\n",
    "                top_k_indices, top_k_scores = topk[1],topk[0]                    \n",
    "                top_k_indices = top_k_indices.view(self.beam_size,batch_size)\n",
    "                top_k_scores = top_k_scores.view(self.beam_size,batch_size)\n",
    "\n",
    "                for new_word_batch, new_score_batch in zip(top_k_indices, top_k_scores):                    \n",
    "                    new_word_batch, new_score_batch = new_word_batch.view(batch_size,1),new_score_batch.view(batch_size,1)\n",
    "                    new_child_batch = t.cat([x_target,new_word_batch],1).detach()                    \n",
    "                   \n",
    "                    batch_parent_score = self.beam_scores[:,it].reshape((self.batch_size,1))\n",
    "                    batch_acc_score =  batch_parent_score + new_score_batch.data.cpu().numpy()        \n",
    "                \n",
    "                    if len(collective_children) > 0:\n",
    "                        collective_children = np.hstack((collective_children, new_child_batch.data.cpu().numpy())) \n",
    "                        #Add the corresponding beam element's score with the new score and stack it.\n",
    "                        collective_scores   = np.hstack((collective_scores, batch_acc_score ))              \n",
    "                    else:\n",
    "                        collective_children, collective_scores = new_child_batch.data.cpu().numpy(),batch_acc_score               \n",
    "\n",
    "                     \n",
    "            #At the end of a for loop collective children, collective scores \n",
    "            #will look a numpy array of tensors.            \n",
    "            current_beam_length = 1 #Means only start elem is there.\n",
    "            if len(self.beam)!= 1:\n",
    "                current_beam_length = self.beam.shape[1]                \n",
    "                  \n",
    "            collective_children = collective_children.reshape((batch_size, current_beam_length*self.beam_size, \n",
    "                                                               int(collective_children.shape[1]/\n",
    "                                                                   current_beam_length/self.beam_size)\n",
    "                                                             ))\n",
    "            \n",
    "            if collective_children.shape[1] == self.beam_size:  #Happens the first time.\n",
    "                self.beam = collective_children  \n",
    "                self.beam_scores = collective_scores\n",
    "                \n",
    "            else:\n",
    "                self.beam = deepcopy(np.zeros((batch_size,self.beam_size,collective_children.shape[2])))\n",
    "                for i in range(batch_size):\n",
    "                    #Since argsort gives ascending order\n",
    "                    best_scores_indices = np.argsort(-1*collective_scores[i])[:self.beam_size]  \n",
    "                    for key,index in enumerate(best_scores_indices):\n",
    "                        self.beam[i][key][:] = collective_children[i][index]\n",
    "                        self.beam_scores[i][key] = collective_scores[i][index]\n",
    "            \n",
    "            terminate_beam = True\n",
    "            \n",
    "            for x in self.beam:\n",
    "                    for c in x:\n",
    "                        if EOS_TOKEN not in c:\n",
    "                            terminate_beam = False\n",
    "                            break\n",
    "                    if not terminate_beam:\n",
    "                        break   \n",
    "            #import pdb; pdb.set_trace()\n",
    "            assert(self.beam.shape == (batch_size,self.beam_size,time+2))\n",
    "\n",
    "            time += 1\n",
    "            print(time)\n",
    "        return self.beam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing LSTMA\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utils import load_model\n",
    "params['beam_size'] = 1\n",
    "#from translation_models import LSTMA\n",
    "from const import *\n",
    "with open('LSTMA/1.params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "lstma = LSTMA(params).cuda()\n",
    "load_model(lstma, 'LSTMA/1.pytorch', cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-96-94db9b503d78>(180)translate_beam()\n",
      "-> hidden = self.init_hidden(None, 'enc')\n",
      "(Pdb) continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:180: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "val_iter.batch_size = 64\n",
    "for batch in val_iter:\n",
    "    break\n",
    "pred_beam = lstma.translate_beam(batch.src.transpose(0,1).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_iter.batch_size = 64\n",
    "for batch in val_iter:\n",
    "    break\n",
    "pred = lstma.translate(batch.src.transpose(0,1).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'thanks',\n",
       " \"'re\",\n",
       " 'to',\n",
       " 'beginning',\n",
       " 'coal',\n",
       " 'wanted',\n",
       " 'can',\n",
       " 'make',\n",
       " 'ca',\n",
       " 'did',\n",
       " 'George',\n",
       " 'can',\n",
       " 'make',\n",
       " 'student',\n",
       " 'way',\n",
       " 'Thanks',\n",
       " 'near',\n",
       " 'yours',\n",
       " '10,000',\n",
       " 'older']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[EN.vocab.itos[int(x)] for x in pred[6].data.cpu().numpy()]\n",
    "[EN.vocab.itos[int(x)] for x in pred_beam[6][2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'The',\n",
       " 'most',\n",
       " 'important',\n",
       " 'thing',\n",
       " 'is',\n",
       " 'passion',\n",
       " '.',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[EN.vocab.itos[x] for x in batch.trg.transpose(0,1)[6].data.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch in val_iter:\n",
    "    pred = lstma(batch.src.transpose(0,1).cuda(), batch.trg.transpose(0,1).cuda()).max(2)[1]\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
